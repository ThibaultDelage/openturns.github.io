
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Sparse least squares polynomial metamodel &#8212; OpenTURNS  documentation</title>
    <link rel="stylesheet" href="../../_static/openturns.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Kriging" href="kriging.html" />
    <link rel="prev" title="Least squares polynomial response surface" href="polynomial_least_squares.html" />
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400,700'
          rel='stylesheet' type='text/css' />
 

  </head><body>
<div class="pageheader">
  <ul>
    <li><a href="http://www.openturns.org/">Home</a></li>
    <li><a href="../../install.html">Get it</a></li>
    <li><a href="../../contents.html">Doc</a></li>
    <li><a href="https://github.com/openturns">Code</a></li>
    <li><a href="http://trac.openturns.org">Bugs</a></li>
  </ul>
  <a href="../../index.html">
    <h1>
      <img src="../../_static/logo-openturns-wo-bg.png" alt="" width=100px height=100px />
      OpenTURNS
    </h1>
    <h2> An Open source initiative for the Treatment of Uncertainties, Risks'N Statistics</h2>
  </a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="kriging.html" title="Kriging"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="polynomial_least_squares.html" title="Least squares polynomial response surface"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTURNS  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../contents.html" >Contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../theory.html" >Reference guide</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="meta_modeling.html" accesskey="U">Meta modeling</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="polynomial_least_squares.html"
                        title="previous chapter">Least squares polynomial response surface</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="kriging.html"
                        title="next chapter">Kriging</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/theory/meta_modeling/polynomial_sparse_least_squares.rst"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="sparse-least-squares-polynomial-metamodel">
<h1>Sparse least squares polynomial metamodel<a class="headerlink" href="#sparse-least-squares-polynomial-metamodel" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line">The response of the model under consideration may be globally
approximated by a multivariate polynomial. In this setup, the response
is characterized by a finite number of coefficients which have to be
estimated. This may be achieved by least squares. However, this
method cannot be applied if the number of unknown coefficients is of
similar size to the number of model evaluations, or even possibly
greater. Indeed, the resulting underdetermined least squares problem
would be ill-posed.</div>
<div class="line">To overcome this difficulty, <em>sparse least squares</em> schemes may be
employed (they are also known as <em>variable selection</em> techniques in
statistics). These methods are aimed at identifying a small set of
significant coefficients in the model response approximation.
Eventually, a <em>sparse</em> polynomial response surface, i.e. a polynomial
which only contains a low number of non-zero coefficients, is obtained
by means of a reduced number of possibly costly model evaluations. In
the sequel, we focus on sparse regression schemes based on
<img class="math" src="../../_images/math/f1a53c37f06e360e76b821c9b4b52bd0c6ae6830.svg" alt="L_1"/>-penalization.</div>
<div class="line">Actually the proposed approaches do not provide only one
approximation, but a <em>collection</em> of less and less sparse
approximations. Eventually an optimal approximation may be retained
with regard to a given criterion.</div>
</div>
<p><strong>Context</strong></p>
<p>Consider the mathematical model <img class="math" src="../../_images/math/5e4fd0b7c0b9d8750ca8bc577ddb169a78f8b00f.svg" alt="h"/> of a physical system depending
on <img class="math" src="../../_images/math/4056a1b2001e6226d1044246e6813d2a3c90eb94.svg" alt="n_X"/> input parameters
<img class="math" src="../../_images/math/7f5ff13a5a57a5d848ab11025f57bec2ce8f7c7c.svg" alt="\underline{x} = (x_{1},\dots,x_{n_X})^{\textsf{T}}"/>. Note that
these input variables are assumed to be deterministic in this section.
The model response may be approximated by a polynomial as follows:</p>
<div class="math" id="equation-5-2-1">
<p><span class="eqno">(1)<a class="headerlink" href="#equation-5-2-1" title="Permalink to this equation">¶</a></span><img src="../../_images/math/98240a3a400885aabe0d5a07a6a9c163a3b9928f.svg" alt="h(\underline{X}) \, \, \approx \, \, \widehat{h}(\underline{x}) \, \, = \, \, \sum_{j=0}^{P-1} \; a_{j} \; \psi_{j}(\underline{x})"/></p>
</div><div class="line-block">
<div class="line">Let us consider a set of values taken by the input vector (i.e. an
experimental design)
<img class="math" src="../../_images/math/e32845eb4de8675facf5dfa43bb2916ac7fc643e.svg" alt="\underline{\underline{\cX}} = (\underline{x}^{(1)},\dots,\underline{x}^{(N)})^{\textsf{T}}"/>
as well as the vector
<img class="math" src="../../_images/math/03861b4c743ef074dc00a38f8986b30b8e36397c.svg" alt="\underline{\cY} = (h(\underline{x}^{(1)}),\dots,h(\underline{x}^{(N)}))^{\textsf{T}} =  (y^{(1)},\dots,y^{(N)})^{\textsf{T}}"/>
of the corresponding model evaluations. It is assumed that the number
of terms <img class="math" src="../../_images/math/4c21bd0a8df01ae42f62d11fb29cb9ae23746161.svg" alt="P"/> in the polynomial basis is of similar size to
<img class="math" src="../../_images/math/590e62c4db8a892c9b9a0c6f58bb7e2f1ec99e9b.svg" alt="N"/>, or even possibly significantly larger than <img class="math" src="../../_images/math/590e62c4db8a892c9b9a0c6f58bb7e2f1ec99e9b.svg" alt="N"/>. In
such a situation it is not possible to compute the polynomial
coefficients by ordinary least squares, since the corresponding system
is ill-posed. Methods that may be employed as an alternative are
outlined in the sequel.</div>
<div class="line"><strong>LASSO</strong></div>
</div>
<div class="line-block">
<div class="line">The so-called <em>LASSO</em> (for <em>Least absolute shrinkage and selection
operator</em>) method is based on a <img class="math" src="../../_images/math/7e0ccc7156660d78324f8d34b4faddd1bb0e07b6.svg" alt="\cL^{1}"/>-penalized regression
as follows:</div>
</div>
<blockquote>
<div><div class="math" id="equation-5-2-5">
<p><span class="eqno">(2)<a class="headerlink" href="#equation-5-2-5" title="Permalink to this equation">¶</a></span><img src="../../_images/math/a8c65e992c3e74d3a0df34574c09274c9f04e7ae.svg" alt="\textrm{Minimize} \quad \quad \sum_{i=1}^{N} \; \left( h(\underline{x}^{(i)}) \; - \; \underline{a}^{\textsf{T}} \underline{\psi}(\underline{x}^{(i)})  \right)^{2}
     \, + \,  C \; \|\underline{a}\|_{1}^{2}"/></p>
</div><p>where <img class="math" src="../../_images/math/f5eaae4007cdee099bd025138f8bc09bde370055.svg" alt="\|\underline{a}\|_{1} = \sum_{j=0}^{P-1} |a_{j}|"/> and
<img class="math" src="../../_images/math/672a37f589017d9850b03c88d416135b30d4f284.svg" alt="C"/> is a non negative constant. A nice feature of LASSO is that
it provides a <em>sparse</em> metamodel, i.e. it discards insignificant
variables from the set of predictors. The obtained response surface is
all the sparser since the value of the tuning parameter <img class="math" src="../../_images/math/672a37f589017d9850b03c88d416135b30d4f284.svg" alt="C"/> is
high.</p>
</div></blockquote>
<div class="line-block">
<div class="line">For a given <img class="math" src="../../_images/math/422617d263ac48d8382f6a831b19f470a09f8816.svg" alt="C\geq 0"/>, the solving procedure may be implemented
via quadratic programming. Obtaining the whole set of coefficient
estimates for <img class="math" src="../../_images/math/672a37f589017d9850b03c88d416135b30d4f284.svg" alt="C"/> varying from 0 to a maximum value may be
computationally expensive though since it requires solving the
optimization problem for a fine grid of values of <img class="math" src="../../_images/math/672a37f589017d9850b03c88d416135b30d4f284.svg" alt="C"/>.</div>
<div class="line"><strong>Forward stagewise regression</strong></div>
</div>
<p>Another procedure, known as <em>forward stagewise regression</em>, appears to
be different from LASSO, but turns out to provide similar results. The
procedure first picks the predictor that is most correlated with the
vector of observations. However, the value of the corresponding
coefficient is only increased by a small amount. Then the predictor with
largest correlation with the current residual (possible the same term as
in the previous step) is picked, and so on. Let us introduce the vector
notation:</p>
<div class="math">
<p><img src="../../_images/math/1bb9ff334a97906605cd2153f346883671d5a3f9.svg" alt="\underline{\psi}_{j} \, \, = \, \, (\psi_{j}(\underline{x}^{(1)}) , \dots, \psi_{j}(\underline{x}^{(N)}) )^{\textsf{T}}"/></p>
</div><p>The forward stagewise algorithm is outlined below:</p>
<ol class="arabic simple">
<li>Start with <img class="math" src="../../_images/math/94fe58b2d96f5b08a605c72049855a5d783893fb.svg" alt="\underline{R} = \cY"/> and
<img class="math" src="../../_images/math/8be8a80b9d7f31672906132c3ee12e7d677027d7.svg" alt="a_{0} = \dots = a_{P-1} = 0"/>.</li>
<li>Find the predictor <img class="math" src="../../_images/math/3d9d710b9aab28ab5d562f3ef15c1b129d62b4ab.svg" alt="\underline{\psi}_{j}"/> that is most
correlated with <img class="math" src="../../_images/math/36dc1659aa445fc32556afd06e94b3781acf9ff0.svg" alt="\underline{R}"/>.</li>
<li>Update <img class="math" src="../../_images/math/8d13e848b38c02bb6b6b65d3d51ea2e306da638a.svg" alt="\hat{a}_{j} = \hat{a}_{j} + \delta_{j}"/>, where
<img class="math" src="../../_images/math/40b0b2f2d052859b14b56988c9f8f8c81477450f.svg" alt="\delta_{j} = \varepsilon \cdot \mbox{sign}(\underline{\psi}_{j}^{\textsf{T}} \underline{R} )"/>.</li>
<li>Update
<img class="math" src="../../_images/math/eced529e7dc9d47e6f6206a39708ae01e4641f5e.svg" alt="\underline{R} =  \underline{R} - \delta_{j} \underline{\psi}_{j}"/>
and repeats Steps&nbsp;2 and 3 until no predictor has any correlation with
<img class="math" src="../../_images/math/36dc1659aa445fc32556afd06e94b3781acf9ff0.svg" alt="\underline{R}"/>.</li>
</ol>
<div class="line-block">
<div class="line">Note that parameter <img class="math" src="../../_images/math/94961bcbae85214199b1565307343ead7e36bc60.svg" alt="\varepsilon"/> has to be set equal to a small
value in practice, say <img class="math" src="../../_images/math/4e0b12b2364aa5a6524e9546f1f38845dfbbe984.svg" alt="\varepsilon=0.01"/>. This procedure is
known to be more stable than traditional stepwise regression.</div>
<div class="line"><strong>Least Angle Regression</strong></div>
</div>
<p><em>Least Angle Regression</em> (LAR) may be viewed as a version of forward
stagewise that uses mathematical derivations to speed up the
computations. Indeed, instead of taking many small steps with the basis
term most correlated with current residual <img class="math" src="../../_images/math/36dc1659aa445fc32556afd06e94b3781acf9ff0.svg" alt="\underline{R}"/>, the
related coefficient is directly increased up to the point where some
other basis predictor has as much correlation with
<img class="math" src="../../_images/math/36dc1659aa445fc32556afd06e94b3781acf9ff0.svg" alt="\underline{R}"/>. Then the new predictor is entered, and the
procedure is continued. The LAR algorithm is detailed below:</p>
<ol class="arabic simple">
<li>Initialize the coefficients to <img class="math" src="../../_images/math/68ebe5bca3a03acb39b33576a9a8966fe85da9f2.svg" alt="a_{0},\dots,a_{P-1} = 0"/>. Set
the initial residual equal to the vector of observations <img class="math" src="../../_images/math/87cd59be13cc0767f3dedf964875e9e5aea66ebb.svg" alt="\cY"/>.</li>
<li>Find the vector <img class="math" src="../../_images/math/3d9d710b9aab28ab5d562f3ef15c1b129d62b4ab.svg" alt="\underline{\psi}_{j}"/> which is most correlated
with the current residual.</li>
<li>Move <img class="math" src="../../_images/math/3f454a303ff875d7bf00da45e82b3176f919bbbc.svg" alt="a_{j}"/> from 0 toward the least-square coefficient of the
current residual on <img class="math" src="../../_images/math/3d9d710b9aab28ab5d562f3ef15c1b129d62b4ab.svg" alt="\underline{\psi}_{j}"/>, until some other
predictor <img class="math" src="../../_images/math/12f0421b7b4ebdf552551861f3520116ddd71b01.svg" alt="\underline{\psi}_{k}"/> has as much correlation with
the current residual as does <img class="math" src="../../_images/math/3d9d710b9aab28ab5d562f3ef15c1b129d62b4ab.svg" alt="\underline{\psi}_{j}"/>.</li>
<li>Move jointly <img class="math" src="../../_images/math/6059c322c479a548fc9c4ccefdac6ce6145ced14.svg" alt="(a_{j} , a_{k})^{\textsf{T}}"/> in the direction
defined by their joint least-square coefficient of the current
residual on <img class="math" src="../../_images/math/5b30ae76103e9e832c1df419fc67c111a4331241.svg" alt="(\underline{\psi}_{j},\underline{\psi}_{k})"/>,
until some other predictor <img class="math" src="../../_images/math/5ec421be5a10e6902e81561f619ec201870b1402.svg" alt="\underline{\psi}_{l}"/> has as much
correlation with the current residual.</li>
<li>Continue this way until <img class="math" src="../../_images/math/19e5ae708bc1968b6c2b2e796bc4d7247f217b3e.svg" alt="m = \min(P,N-1)"/> predictors have been
entered.</li>
</ol>
<div class="line-block">
<div class="line">Steps 2 and 3 correspond to a “move” of the <em>active</em> coefficients
toward their least-square value. It corresponds to an updating of the
form
<img class="math" src="../../_images/math/19728561b0089ff828ffc4da423276910397fc10.svg" alt="\hat{\underline{a}}^{(k+1)} = \hat{\underline{a}}^{(k)} + \gamma^{(k)} \tilde{\underline{w}}^{(k)}"/>.
Vector <img class="math" src="../../_images/math/fe274d29f567b2e5518eca0225a1f782c300be89.svg" alt="\tilde{\underline{w}}^{(k)}"/> and coefficient
<img class="math" src="../../_images/math/93d4a62c7e14b16ff6c7298e7fc74a1922625459.svg" alt="\gamma^{(k)}"/> are referred to as the LAR <em>descent direction</em>
and <em>step</em>, respectively. Both quantities may be derived
algebraically.</div>
<div class="line">Note that if <img class="math" src="../../_images/math/dfdb687b3310b1f3570014457a79954371f4a4e6.svg" alt="N \geq P"/>, then the last step of LAR provides the
ordinary least-square solution. It is shown that LAR is noticeably
efficient since it only requires the computational cost of ordinary
least-square regression on <img class="math" src="../../_images/math/4c21bd0a8df01ae42f62d11fb29cb9ae23746161.svg" alt="P"/> predictors for producing a
<em>collection</em> of <img class="math" src="../../_images/math/93f1437a26e1e140120e1891ad60149cdf21f33d.svg" alt="m"/> metamodels.</div>
<div class="line"><strong>LASSO and Forward Stagewise as variants of LAR</strong></div>
</div>
<p>It has been shown that with only one modification, the LAR procedure
provides in one shot the entire paths of LASSO solution coefficients as
the tuning parameter <img class="math" src="../../_images/math/672a37f589017d9850b03c88d416135b30d4f284.svg" alt="C"/> in Eq.([eq:5-2.5]) is increased from 0 up
to a maximum value. The modified algorithm is as follows:</p>
<ul class="simple">
<li>Run the LAR procedure from Steps&nbsp;1 to 4.</li>
<li>If a non zero coefficient hits zero, discard it from the current
metamodel and recompute the current joint least-square direction.</li>
<li>Continue this way until <img class="math" src="../../_images/math/19e5ae708bc1968b6c2b2e796bc4d7247f217b3e.svg" alt="m = \min(P,N-1)"/> predictors have been
entered.</li>
</ul>
<div class="line-block">
<div class="line">Note that the LAR-based LASSO procedure may take more than <img class="math" src="../../_images/math/93f1437a26e1e140120e1891ad60149cdf21f33d.svg" alt="m"/>
steps since the predictors are allowed to be discarded and introduced
later again into the metamodel. In a similar fashion, a limiting
version of the forward stagewise method when <img class="math" src="../../_images/math/fbe247d89cfce8a3ec3a3ffe61b2b0458d3db3cc.svg" alt="\varepsilon \to 0"/>
may be obtained by slightly modifying the original LAR algorithm. In
the literature, one commonly uses the label LARS when referring to all
these LAR-based algorithms (with <em>S</em> referring to <em>Stagewise</em> and
<em>LASSO</em>).</div>
<div class="line"><strong>Selection of the optimal LAR solution</strong></div>
</div>
<p>The LAR-based approaches (i.e. original LAR, LASSO and forward
stagewise) provide a <em>collection</em> of less and less sparse PC
approximations. The accuracy of each PC metamodel may be assessed by
cross validation. Eventually the PC representation associated with the
lowest error estimate is retained.</p>
<div class="topic">
<p class="topic-title first">API:</p>
<ul class="simple">
<li>See <a class="reference internal" href="../../user_manual/response_surface/_generated/openturns.LARS.html#openturns.LARS" title="openturns.LARS"><code class="xref py py-class docutils literal notranslate"><span class="pre">LARS</span></code></a></li>
<li>See <a class="reference internal" href="../../user_manual/response_surface/_generated/openturns.FunctionalChaosAlgorithm.html#openturns.FunctionalChaosAlgorithm" title="openturns.FunctionalChaosAlgorithm"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunctionalChaosAlgorithm</span></code></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li>See <a class="reference internal" href="../../examples/meta_modeling/fieldfunction_metamodel.html"><span class="doc">Metamodel of a field function</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><ol class="first upperalpha" start="2">
<li>Efron, T. Hastie, I. Johnstone, and R. Tibshirani, 2004, “Least angle regression”, Annals of Statistics 32, 407–499.</li>
</ol>
</li>
<li><ol class="first upperalpha" start="20">
<li>Hastie, J. Taylor, R. Tibshirani, and G. Walther, 2007, “Forward stagewise regression and the monotone Lasso”, Electronic J. Stat. 1, 1–29.</li>
</ol>
</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="kriging.html" title="Kriging"
             >next</a> |</li>
        <li class="right" >
          <a href="polynomial_least_squares.html" title="Least squares polynomial response surface"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTURNS  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../contents.html" >Contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../theory.html" >Reference guide</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="meta_modeling.html" >Meta modeling</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2005-2018 Airbus-EDF-IMACS-Phimeca.
      Last updated on Jan 01, 2018.
    </div>
  </body>
</html>
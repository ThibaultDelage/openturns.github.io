
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Bayesian calibration &#8212; OpenTURNS  documentation</title>
    <link rel="stylesheet" href="../../_static/openturns.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="The Metropolis-Hastings Algorithm" href="metropolis_hastings.html" />
    <link rel="prev" title="Maximum Likelihood Principle" href="maximum_likelihood.html" />
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400,700'
          rel='stylesheet' type='text/css' />
 

  </head><body>
<div class="pageheader">
  <ul>
    <li><a href="http://www.openturns.org/">Home</a></li>
    <li><a href="../../install.html">Get it</a></li>
    <li><a href="../../contents.html">Doc</a></li>
    <li><a href="https://github.com/openturns">Code</a></li>
    <li><a href="http://trac.openturns.org">Bugs</a></li>
  </ul>
  <a href="../../index.html">
    <h1>
      <img src="../../_static/logo-openturns-wo-bg.png" alt="" width=100px height=100px />
      OpenTURNS
    </h1>
    <h2> An Open source initiative for the Treatment of Uncertainties, Risks'N Statistics</h2>
  </a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="metropolis_hastings.html" title="The Metropolis-Hastings Algorithm"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="maximum_likelihood.html" title="Maximum Likelihood Principle"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTURNS  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../contents.html" >Contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../theory.html" >Theory</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="data_analysis.html" accesskey="U">Data analysis</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="maximum_likelihood.html"
                        title="previous chapter">Maximum Likelihood Principle</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="metropolis_hastings.html"
                        title="next chapter">The Metropolis-Hastings Algorithm</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/theory/data_analysis/bayesian_calibration.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="bayesian-calibration">
<span id="id1"></span><h1>Bayesian calibration<a class="headerlink" href="#bayesian-calibration" title="Permalink to this headline">Â¶</a></h1>
<p>We consider a computer model <img class="math" src="../../_images/math/5e4fd0b7c0b9d8750ca8bc577ddb169a78f8b00f.svg" alt="h"/> (<em>i.e.</em> a deterministic function)
to calibrate:</p>
<div class="math">
<p><img src="../../_images/math/18b7acc5ba89998c1a6e0ddfd08e785d458666d6.svg" alt="\begin{aligned}
    \vect{z} = h(\vect{x}, \vect{\theta}_h),
  \end{aligned}"/></p>
</div><p>where</p>
<ul class="simple">
<li><img class="math" src="../../_images/math/93f92d0c4cd01962a940648d4eb0079bc15c4117.svg" alt="\vect{x} \in \Rset^{d_x}"/> is the input vector;</li>
<li><img class="math" src="../../_images/math/1254c6f5e6617d16becf4382bbb95099da57e126.svg" alt="\vect{z} \in \Rset^{d_z}"/> is the output vector;</li>
<li><img class="math" src="../../_images/math/96dfa7c7b966466986fb090af72c053057518b6f.svg" alt="\vect{\theta}_h \in \Rset^{d_h}"/> are the unknown parameters of
<img class="math" src="../../_images/math/5e4fd0b7c0b9d8750ca8bc577ddb169a78f8b00f.svg" alt="h"/> to calibrate.</li>
</ul>
<p>Our goal here is to estimate <img class="math" src="../../_images/math/07be66284bbb4f59f17abadf50d39c26e08348da.svg" alt="\vect{\theta}_h"/>, based on a certain
set of <img class="math" src="../../_images/math/3492afb5563cde1aa48c3366d238465a2fd06173.svg" alt="n"/> inputs <img class="math" src="../../_images/math/a7ce63bbbce324009f9f4983017351001899cfaf.svg" alt="(\vect{x}^1, \ldots, \vect{x}^n)"/> (an
experimental design) and some associated observations
<img class="math" src="../../_images/math/498d6e19fad6c11af39ec99ff370fe939b789619.svg" alt="(\vect{y}^1, \ldots, \vect{y}^n)"/> which are regarded as the
realizations of some random vectors
<img class="math" src="../../_images/math/4ef96c24fd6154f2bfb039e5381727b00f7414c3.svg" alt="(\vect{Y}^1, \ldots, \vect{Y}^n)"/>, such that, for all <img class="math" src="../../_images/math/83a1929c1cd341d9476f20e34f0ca7ab802344c3.svg" alt="i"/>,
the distribution of <img class="math" src="../../_images/math/6d20aae91fec7ac2c9d84c7f054c3c6720396848.svg" alt="\vect{Y}^i"/> depends on
<img class="math" src="../../_images/math/e1b6734fc2f0efe8a0e8103cc987f11d2fcae658.svg" alt="\vect{z}^i = h(\vect{x}^i, \vect{\theta}_h)"/>. Typically,
<img class="math" src="../../_images/math/783564c72daea1a7f40bbd3d314910128e6039f0.svg" alt="\vect{Y}^i = \vect{z}^i + \vect{\varepsilon}^i"/> where
<img class="math" src="../../_images/math/a164fc3542bd8c5c6e7fde926108dd49a238f902.svg" alt="\vect{\varepsilon}^i"/> is a random measurement error.</p>
<p>For the sake of clarity, lower case letters are used for both random
variables and realizations in the following (the notation does not
distinguish the two anymore), as usual in the bayesian literature.</p>
<p>In fact, the bayesian procedure which is implemented allows to infer
some unknown parameters <img class="math" src="../../_images/math/960a4809b3516a3e859d2b58f3424891d926830e.svg" alt="\vect{\theta}\in\Rset^{d_\theta}"/> from
some data <img class="math" src="../../_images/math/ca923e149f0c7da17a37c5a2651fd44fa024eae0.svg" alt="\mat{y} = (\vect{y}^1, \ldots, \vect{y}^n)"/> as soon as
the conditional distribution of each <img class="math" src="../../_images/math/9e92027e1d91f60a8178bddb620e283a289826fe.svg" alt="\vect{y}^i"/> given
<img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/> is specified. Therefore <img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/> can
be made up with some computer model parameters <img class="math" src="../../_images/math/07be66284bbb4f59f17abadf50d39c26e08348da.svg" alt="\vect{\theta}_h"/>
together with some others <img class="math" src="../../_images/math/77d90c43715511f163500cb5bef56e2530f30f99.svg" alt="\vect{\theta}_\varepsilon"/>:
<img class="math" src="../../_images/math/75effb409c7c277ab7b0cd74b3bea1c1382c8d49.svg" alt="\vect{\theta} = \Tr{(\Tr{\vect{\theta}_h}, \Tr{\vect{\theta}_\varepsilon})}"/>.
For example, <img class="math" src="../../_images/math/77d90c43715511f163500cb5bef56e2530f30f99.svg" alt="\vect{\theta}_\varepsilon"/> may represent the unknown
standard deviation <img class="math" src="../../_images/math/21f69c83a86be9c7d80bf1d39f76d158a95228af.svg" alt="\sigma"/> of an additive centered gaussian
measurement error affecting the data (see the example hereafter).
Besides the procedure can be used to estimate the parameters of a
distribution from direct observations (no computer model to calibrate:
<img class="math" src="../../_images/math/0451c0d3454c21940372f969794f22593f0c91c2.svg" alt="\vect{\theta} = \vect{\theta}_\varepsilon"/>).</p>
<p>More formally, the likelihood <img class="math" src="../../_images/math/4c396acd5f71e49670feccfca98e7fdfdb6db635.svg" alt="L(\mat{y} | \vect{\theta})"/> is
defined by, firstly, a family
<img class="math" src="../../_images/math/c24d0cbab5fe123598c3d0a886e840df18038046.svg" alt="\{\cP_{\vect{w}}, \vect{w} \in \Rset^{d_w}\}"/> of probability
distributions parametrized by <img class="math" src="../../_images/math/04138b5ca90d217817fc8b849dfa9285ecb57385.svg" alt="\vect{w}"/>, which is specified in
practice by a conditional distribution <img class="math" src="../../_images/math/7116ae65194b8d60f33fae80366ea122cb5bb59d.svg" alt="f(.|\vect{w})"/> given
<img class="math" src="../../_images/math/04138b5ca90d217817fc8b849dfa9285ecb57385.svg" alt="\vect{w}"/> (<img class="math" src="../../_images/math/f4a0d602d0175d7f069a2706ccb2b7150c87bbc2.svg" alt="f"/> is a PDF or a probability mass function),
and, secondly, a function
<img class="math" src="../../_images/math/5cb0c680d37823c884cdefe67e39a1d62e209cf4.svg" alt="g:\Rset^{d_\theta} \longrightarrow \Rset^{n\,d_w}"/> such that
<img class="math" src="../../_images/math/989449a7f56a71ce963f28ef6cd4da74ebeddfa3.svg" alt="g(\theta) = \Tr{(\Tr{g^1(\vect{\theta})}, \ldots, \Tr{g^n(\vect{\theta})})}"/>
which enables to express the parameter <img class="math" src="../../_images/math/30c6bc99b2ea4643244be290393528bfaadf3a68.svg" alt="\vect{w}^i"/> of the i-th
observation <img class="math" src="../../_images/math/932becd3143ffe1d3b2000b471eea030298bfe93.svg" alt="\vect{y}^i \sim f(.|\vect{w}^i)"/> in function of
<img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/>: <img class="math" src="../../_images/math/f9602cd723a3b17661e2685f5909e53bc032354f.svg" alt="g^i(\vect{\theta}) = \vect{w}^i"/> thus
<img class="math" src="../../_images/math/3ca3e59986c51551349f7f94243dec77de863f6b.svg" alt="\vect{y}^i \sim f(.|g^i(\vect{\theta}))"/> and</p>
<div class="math">
<p><img src="../../_images/math/64419454723c1c3df99e3ec60108f3eb2de2a362.svg" alt="\begin{aligned}
    L(\mat{y} | \vect{\theta}) = \prod_{i=1}^n f(\vect{y}^i|g^i(\vect{\theta})).
  \end{aligned}"/></p>
</div><p>Considering the issue of the calibration of some computer model
parameters <img class="math" src="../../_images/math/07be66284bbb4f59f17abadf50d39c26e08348da.svg" alt="\vect{\theta}_h"/>, the full statistical model can be
seen as a two-level hierarchical model, with a single level of latent
variables <img class="math" src="../../_images/math/91dee50a7ea7a248f11c9d4307984681b94ebac2.svg" alt="\vect{z}"/>. A classical example is given by the
nonlinear Gaussian regression model:</p>
<div class="math">
<p><img src="../../_images/math/5aa454b441a71ecd395def39a166edf223d5bd7a.svg" alt="\begin{aligned}
    y_i &amp;=&amp; h(\vect{x}_i|\vect{\theta}_h) + \varepsilon_i,
    \mbox{ where } \varepsilon_i \stackrel{i.i.d.}{\sim} \cN(0, \sigma^2),
    \quad i = 1,\ldots, n.
  \end{aligned}"/></p>
</div><p>It can be implemented with <img class="math" src="../../_images/math/fe0749ea4b2fcb45c20cc4f9e0158e813838f90b.svg" alt="f(.|\Tr{(\mu, \sigma)})"/> the PDF of
the Gaussian distribution <img class="math" src="../../_images/math/9b73722543b422358e7159365adc9cd953858c1a.svg" alt="\cN(\mu, \sigma^2)"/>, with
<img class="math" src="../../_images/math/ae6576363fb4e88dceda71b43b65b2f42ba2b226.svg" alt="g^i(\vect{\theta}) = \Tr{(h(\vect{x}^i, \vect{\theta}_h), \:\sigma)}"/>,
and with <img class="math" src="../../_images/math/d3c63640f3bd3aaa135418056b2f4c65df1a13ca.svg" alt="\vect{\theta} = \vect{\theta}_h"/>, respectively
<img class="math" src="../../_images/math/18c3a845641a77a134a70e32c7e16bc492d19efa.svg" alt="\vect{\theta} = \Tr{(\Tr{\vect{\theta}_h}, \sigma)}"/>, if
<img class="math" src="../../_images/math/21f69c83a86be9c7d80bf1d39f76d158a95228af.svg" alt="\sigma"/> is considered known, respectively unknown.</p>
<p>Given a distribution modelling the uncertainty on <img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/>
prior to the data, Bayesian inference is used to perform the inference
of <img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/>, hence the name Bayesian calibration.</p>
<p>Contrary to the maximum likelihood approach described in <a class="reference internal" href="maximum_likelihood.html#maximum-likelihood"><span class="std std-ref">Maximum Likelihood Principle</span></a>, which
provides a single âbest estimateâ value <img class="math" src="../../_images/math/a755434aece71d75b30568358b67447d5513b2da.svg" alt="\hat{\vect{\theta}}"/>,
together with confidence bounds accounting for the uncertainty remaining
on the true value <img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/>, the Bayesian approach derives a
full distribution of possible values for <img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/> given the
available data <img class="math" src="../../_images/math/4e89f9135a11b29c81f4d6e3d8df4f37af49788f.svg" alt="\mat{y}"/>. Known as the <em>posterior distribution</em> of
<img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/> given the data <img class="math" src="../../_images/math/4e89f9135a11b29c81f4d6e3d8df4f37af49788f.svg" alt="\mat{y}"/>, its density can be
expressed according to Bayesâ theorem:</p>
<div class="math" id="equation-postdistribution">
<p><span class="eqno">(1)<a class="headerlink" href="#equation-postdistribution" title="Permalink to this equation">Â¶</a></span><img src="../../_images/math/e81e7ca64f9517904437b13921a1efd7966e7da9.svg" alt="\begin{aligned}
     \pi(\vect{\theta} | \mat{y}) &amp;=&amp; \frac{L(\mat{y} | \vect{\theta}) \times \pi(\vect{\theta})}{m(\mat{y})},
   \end{aligned}"/></p>
</div><p>where</p>
<ul>
<li><p class="first"><img class="math" src="../../_images/math/4c396acd5f71e49670feccfca98e7fdfdb6db635.svg" alt="L(\mat{y} | \vect{\theta})"/> is the (data) likelihood;</p>
</li>
<li><p class="first"><img class="math" src="../../_images/math/b84f44ce0a51d0ba2d1c783b08b18ad416e9165a.svg" alt="\pi(\vect{\theta})"/> is the so-called <em>prior distribution</em> of
<img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/> (with support <img class="math" src="../../_images/math/ef75dff6b2fd64d5ec784de308bfd9161f6cc5cf.svg" alt="\Theta"/>), which encodes
all possible <img class="math" src="../../_images/math/f9316b2c14de0052a94f2eab15f91b69f1786d35.svg" alt="\vect{\theta}"/> values weighted by their prior
probabilities, before consideration of any experimental data (this
allows for instance to incorporate expert information or known
physical constraints on the calibration parameter)</p>
</li>
<li><p class="first"><img class="math" src="../../_images/math/23ca8d22442c1b94ef9a4a597ff5d422c085cb09.svg" alt="m(\mat{y})"/> is the marginal likelihood:</p>
<div class="math">
<p><img src="../../_images/math/f0ade7c4602bfb6c4ae9a230014d90b771b5cf00.svg" alt="\begin{aligned}
      m(\mat{y}) &amp;=&amp; \displaystyle\int_{\vect{\theta}\in\Theta} L(\mat{y} | \vect{\theta}) \pi(\vect{\theta}) d\vect{\theta},
    \end{aligned}"/></p>
</div></li>
</ul>
<p>which is the necessary normalizing constant ensuring that the
posterior density integrates to <img class="math" src="../../_images/math/f3d44eee68f965e0c3a97b452cb1cc2898b0cfdd.svg" alt="1"/>.</p>
<p>Except in very simple cases, <a class="reference internal" href="#equation-postdistribution">(1)</a> has, in general,
no closed form. Thus, it must be approximated, either using numerical
integration when the parameter space dimension <img class="math" src="../../_images/math/19e86849fb4583c93e671b46533a0f1146696086.svg" alt="d_\theta"/> is low,
or more generally through stochastic sampling techniques known as
Monte-Carlo Markov-Chain (MCMC) methods. See <a class="reference internal" href="metropolis_hastings.html#metropolis-hastings"><span class="std std-ref">The Metropolis-Hastings Algorithm</span></a>.</p>
<div class="topic">
<p class="topic-title first">API:</p>
<ul class="simple">
<li>See <a class="reference internal" href="../../user_manual/_generated/openturns.RandomWalkMetropolisHastings.html#openturns.RandomWalkMetropolisHastings" title="openturns.RandomWalkMetropolisHastings"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomWalkMetropolisHastings</span></code></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li>See <span class="xref std std-doc">/examples/data_analysis/bayesian_calibration</span></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li>Berger, J.O. (1985). <em>Statistical Decision Theory and Bayesian Analysis</em>, Springer.</li>
<li>Marin J.M. &amp; Robert C.P. (2007) <em>Bayesian Core: A Practical Approach to Computational Bayesian Statistics</em>, Springer.</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="metropolis_hastings.html" title="The Metropolis-Hastings Algorithm"
             >next</a> |</li>
        <li class="right" >
          <a href="maximum_likelihood.html" title="Maximum Likelihood Principle"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTURNS  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../contents.html" >Contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../theory.html" >Theory</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="data_analysis.html" >Data analysis</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2005-2018 Airbus-EDF-IMACS-Phimeca.
      Last updated on Jan 01, 2018.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.5.
    </div>
  </body>
</html>
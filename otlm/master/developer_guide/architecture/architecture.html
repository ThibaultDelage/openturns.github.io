
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Linear regression models &#8212; otlm 0.6 documentation</title>
    <link rel="stylesheet" href="../../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Validation" href="../validation/validation.html" />
    <link rel="prev" title="Developer guide" href="../developer_guide.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../validation/validation.html" title="Validation"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../developer_guide.html" title="Developer guide"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">otlm 0.6 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../developer_guide.html" accesskey="U">Developer guide</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="linear-regression-models">
<h1>Linear regression models<a class="headerlink" href="#linear-regression-models" title="Permalink to this headline">¶</a></h1>
<p>Let us consider the general linear regression model:</p>
<div class="math">
<p><img src="../../_images/math/1ebbb57f5b806f2622b17f7a18af9cc9d52a5120.svg" alt="\boxed{Y \,=\, X \,\beta\, +\, \epsilon}"/></p>
</div><p>Where <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/> is the design matrix of explanatory variables of size <img class="math" src="../../_images/math/c5fb32c10bb523b5ec346a8a9e516a66c3e40429.svg" alt="(n \times p)"/>,
<img class="math" src="../../_images/math/107a2558bec41a738d186af09dd02b964e85a7bb.svg" alt="Y"/> is the vector of response values of size <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/>,
<img class="math" src="../../_images/math/927bd180fdf374460bb3f58ab861c4f3aaad7eea.svg" alt="\beta"/> is a vector of unknown parameters to be estimated of size <img class="math" src="../../_images/math/2640e9252aac1782adab24797b02ae248a680467.svg" alt="(p)"/>,
and <img class="math" src="../../_images/math/1d4fb4b23e6113b430a00d3091a84736a35dcced.svg" alt="\epsilon"/> is the error vector of size <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/>; <img class="math" src="../../_images/math/1d4fb4b23e6113b430a00d3091a84736a35dcced.svg" alt="\epsilon"/> is assumed to follow the standard Normal distribution.</p>
<p>We define <img class="math" src="../../_images/math/bc3de144e38982b465acf51459c615cd08029341.svg" alt="G_X"/> the Gram matrix of <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/> of size <img class="math" src="../../_images/math/a788dede5b50f9ae1e949b74501b9c1bc0a4f783.svg" alt="(p\times p)"/>, <img class="math" src="../../_images/math/76531441490dd4e4f665e244fe9eff6ba9406442.svg" alt="A_X"/> its inverse,
and <img class="math" src="../../_images/math/10dcf85389521ab8928b45b595e17525c54c4127.svg" alt="H_X"/> the projection matrix of size <img class="math" src="../../_images/math/21effe8463dfdca7fe64474fc05c8f672f8a8dc3.svg" alt="(n\times n)"/> by:</p>
<div class="math">
<p><img src="../../_images/math/7e3dded93234c0a42ca0826dee6ef33a10902212.svg" alt="G_X \hat{=}X^T X  \quad,\quad  A_X \hat{=}(X^T X)^{-1}  \quad,\quad
H_X \hat{=} X_{}\,\big(X^T_{} \,X_{}\big)^{-1} \,X^T_{}  =  X_{}\,A_X \,X^T_{}"/></p>
</div><p>We define the <em>Log likelihood</em> function by:</p>
<div class="math">
<p><img src="../../_images/math/2296b1445567df297832c772da2c9096326b0758.svg" alt="\log L(\beta,\sigma\mid Y)= -\frac{n}{2}\big(\log(2\pi)+ \log(\sigma^2)\big)- \frac{1}{2\sigma^2}\big(Y-X\beta\big)^T\,\big(Y-X\beta\big)"/></p>
</div><p>The solution which maximizes the <em>Log likelihood</em> function is:</p>
<div class="math" id="equation-beta-sigma-opt">
<p><span class="eqno">(1)<a class="headerlink" href="#equation-beta-sigma-opt" title="Permalink to this equation">¶</a></span><img src="../../_images/math/7f64fe73a66c93db9becb35fa47ed27215fe0e96.svg" alt="\hat{\beta} \,=\, \big(X^T_{} \,X_{}\big)^{-1} \,X^T \, Y
\quad,\quad
\hat{\sigma}^2 = \frac{1}{n}\big(Y-X \,\hat{\beta}\big)^T\,\big(Y-X \,\hat{\beta}\big)"/></p>
</div><p>Using equation <a class="reference internal" href="#equation-beta-sigma-opt">(1)</a>, the maximum <em>Log likelihood</em> turns into:</p>
<div class="math" id="equation-maxloglikelihood">
<p><span class="eqno">(2)<a class="headerlink" href="#equation-maxloglikelihood" title="Permalink to this equation">¶</a></span><img src="../../_images/math/ab765a4be3f36c212790e8ba615ef2c0414617d4.svg" alt="\log L(\hat{\beta},\hat{\sigma}\mid Y)=-\frac{n}{2}\big(\log(2\pi)+ \log(\hat{\sigma}^2)+1\big)
\quad \text{where} \quad
\hat{\sigma}^2 = \frac{1}{n}\big(Y-H_X\,Y\big)^T\,\big(Y-H_X\,Y \big)=\frac{1}{n}\|\,Y-H_X\,Y\,\|^2_2"/></p>
</div><p>The residuals are defined by</p>
<div class="math">
<p><img src="../../_images/math/ec722d114b46f5a5db9df06e05eac96be9afc889.svg" alt="\hat{\epsilon} = Y - H_X Y"/></p>
</div></div>
<div class="section" id="architecture-considerations">
<h1>Architecture considerations<a class="headerlink" href="#architecture-considerations" title="Permalink to this headline">¶</a></h1>
<div class="section" id="dependencies">
<h2>Dependencies<a class="headerlink" href="#dependencies" title="Permalink to this headline">¶</a></h2>
<p>Several dependencies are needed in order to build the module:</p>
<blockquote>
<div><ul class="simple">
<li>OpenTURNS</li>
<li>Sphinx-doc (optional for this doc)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="compilation">
<h2>Compilation<a class="headerlink" href="#compilation" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> otlm
mkdir -p build <span class="o">&amp;&amp;</span> <span class="nb">cd</span> build
cmake <span class="se">\</span>
  -DCMAKE_INSTALL_PREFIX<span class="o">=</span><span class="nv">$PWD</span>/install <span class="se">\</span>
  -DOpenTURNS_DIR<span class="o">=</span><span class="nv">$PWD</span>/../../openturns/build/install/lib/cmake/openturns <span class="se">\</span>
  ..
</pre></div>
</div>
</div>
<div class="section" id="source-code-structure">
<h2>Source code structure<a class="headerlink" href="#source-code-structure" title="Permalink to this headline">¶</a></h2>
<p>This section makes up the general specification design for the linear model stepwise regression analysis
in OpenTURNS.</p>
<div class="section" id="linearmodel">
<h3>LinearModel<a class="headerlink" href="#linearmodel" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="http://openturns.github.io/user_manual/_generated/openturns.LinearModel.html#openturns.LinearModel" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModel</span></code></a> class is outdated and does not follow
current best practices about metamodel classes.
We will introduce <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAlgorithm.html#otlm.LinearModelAlgorithm" title="otlm.LinearModelAlgorithm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelAlgorithm</span></code></a>,
<a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis" title="otlm.LinearModelAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelAnalysis</span></code></a> and <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelResult.html#otlm.LinearModelResult" title="otlm.LinearModelResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelResult</span></code></a>
classes.
All current uses of <a class="reference external" href="http://openturns.github.io/user_manual/_generated/openturns.LinearModel.html#openturns.LinearModel" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModel</span></code></a> have to be modified; it is
used in classes <code class="xref py py-class docutils literal notranslate"><span class="pre">VisualTest</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">CorrelationAnalysis</span></code>
and <code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelTest</span></code>.
Class <a class="reference external" href="http://openturns.github.io/user_manual/_generated/openturns.LinearModelFactory.html#openturns.LinearModelFactory" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelFactory</span></code></a> has to be deprecated.</p>
<div class="figure align-center" id="id4">
<img alt="LinearModelAlgorithm and LinearModelResult classes" src="../../_images/LinearModelAlgorithm.png" />
<p class="caption"><span class="caption-text">LinearModelAlgorithm and LinearModelResult classes</span></p>
</div>
</div>
<div class="section" id="anova-table">
<h3>ANOVA table<a class="headerlink" href="#anova-table" title="Permalink to this headline">¶</a></h3>
<p>It is requested to give access to the following data:</p>
<ul class="simple">
<li>Linear model formula, in a textual form</li>
<li>Residuals</li>
<li>For each factor,<ul>
<li>its coefficient</li>
<li>its standard error</li>
<li>p-value for Student test</li>
</ul>
</li>
<li>Number of degrees of freedom</li>
<li>Coefficients <img class="math" src="../../_images/math/505e39f26912eade8bcf57930c6e30203851e077.svg" alt="R^2"/> and adjusted <img class="math" src="../../_images/math/505e39f26912eade8bcf57930c6e30203851e077.svg" alt="R^2"/></li>
<li>p-value of the Fisher test</li>
<li>normality tests on residuals (Kolmogorov-Smirnov, Anderson-Darling and <img class="math" src="../../_images/math/16ebd49799855c8b9ad9f202e860b2917a717f39.svg" alt="\chi^2"/>)</li>
</ul>
<p>Note: Student test uses standardized residuals</p>
<div class="math">
<p><img src="../../_images/math/6054e630281ecba8ed734851832282a6df5a26c5.svg" alt="\frac{\hat{\beta}_i}{\sqrt{\frac{n}{n-p-1}\left[(X^T X)^{-1}\right]_{i,i}}}"/></p>
</div></div>
<div class="section" id="graphical-diagnostics">
<h3>Graphical diagnostics<a class="headerlink" href="#graphical-diagnostics" title="Permalink to this headline">¶</a></h3>
<p>Several plots are provided by <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis" title="otlm.LinearModelAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelAnalysis</span></code></a> class, see diagram class.</p>
<ul>
<li><p class="first"><a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis.drawResidualsVsFitted" title="otlm.LinearModelAnalysis.drawResidualsVsFitted"><code class="xref py py-func docutils literal notranslate"><span class="pre">drawResidualsVsFitted()</span></code></a> plots standardized residuals
<img class="math" src="../../_images/math/f1826a8d3272bc7491347de7e58f3a08c3709464.svg" alt="\tilde{\epsilon}"/> vs. fitted values, with</p>
<div class="math">
<p><img src="../../_images/math/b1fa5f8de31afef979b9bfdb84dd3d4b64005081.svg" alt="\tilde{\epsilon}_i = \frac{\hat{\epsilon}_i}{\sqrt{\frac{n}{n-p-1}\hat{\sigma}^2 (1-H_{i,i})}}"/></p>
</div></li>
<li><p class="first"><a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis.drawQQplot" title="otlm.LinearModelAnalysis.drawQQplot"><code class="xref py py-func docutils literal notranslate"><span class="pre">drawQQplot()</span></code></a> plots <img class="math" src="../../_images/math/22ba20abc91278284c2a1474e9eafc84de39a548.svg" alt="\sqrt{|\tilde{\epsilon}_i|}"/>
vs. theoretical quantiles.</p>
</li>
<li><p class="first"><a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis.drawScaleLocation" title="otlm.LinearModelAnalysis.drawScaleLocation"><code class="xref py py-func docutils literal notranslate"><span class="pre">drawScaleLocation()</span></code></a> plots <img class="math" src="../../_images/math/e9e9cb3cc30ab25f8d667f63970ba7f9d6de7764.svg" alt="\sqrt{\tilde{\epsilon}_i}"/>
vs. fitted values.</p>
</li>
<li><p class="first"><a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis.drawCookDistance" title="otlm.LinearModelAnalysis.drawCookDistance"><code class="xref py py-func docutils literal notranslate"><span class="pre">drawCookDistance()</span></code></a> plots an histogram of Cook’s distance</p>
<div class="math">
<p><img src="../../_images/math/93b2fdff27db6397698073f71ac0220af7ac00bd.svg" alt="D_i = \frac{\tilde{\epsilon}_i^2}{p} \left(\frac{H_{i,i}}{1-H_{i,i}}\right)"/></p>
</div></li>
<li><p class="first"><a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis.drawResidualsVsLeverages" title="otlm.LinearModelAnalysis.drawResidualsVsLeverages"><code class="xref py py-func docutils literal notranslate"><span class="pre">drawResidualsVsLeverages()</span></code></a> plots standardized
residuals <img class="math" src="../../_images/math/f1826a8d3272bc7491347de7e58f3a08c3709464.svg" alt="\tilde{\epsilon}"/> vs leverages</p>
<div class="math">
<p><img src="../../_images/math/6f7450229f058c19d38b08f8688ddb55ae0136bc.svg" alt="h_i = H_{i,i}"/></p>
</div><p>Moreover, this plot also contains contour plot of function</p>
<div class="math">
<p><img src="../../_images/math/f314116c18be9d35d203aa50475074f191780f49.svg" alt="D(x,y) = \frac{y^2}{p}\left(\frac{x}{1-x}\right)"/></p>
</div><p>for levels 0.5 and 1.</p>
</li>
<li><p class="first"><a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis.drawCookVsLeverages" title="otlm.LinearModelAnalysis.drawCookVsLeverages"><code class="xref py py-func docutils literal notranslate"><span class="pre">drawCookVsLeverages()</span></code></a> plots Cook’s distance
<img class="math" src="../../_images/math/f1826a8d3272bc7491347de7e58f3a08c3709464.svg" alt="\tilde{\epsilon}"/> vs <img class="math" src="../../_images/math/7b1b7517008bbd90cbeb4836b9e5c207793a0401.svg" alt="\frac{h_i}{1-h_i}"/>.</p>
<div class="math">
<p><img src="../../_images/math/6f7450229f058c19d38b08f8688ddb55ae0136bc.svg" alt="h_i = H_{i,i}"/></p>
</div><p>Moreover, this plot also contains isolines of function</p>
<div class="math">
<p><img src="../../_images/math/f785b202dc1f5c71e77b68fd414ba48dc7767b79.svg" alt="\frac{py}{x}"/></p>
</div></li>
</ul>
</div>
<div class="section" id="stepwise-regression-methods">
<h3>Stepwise regression methods<a class="headerlink" href="#stepwise-regression-methods" title="Permalink to this headline">¶</a></h3>
<p>The stepwise regression method consists in choosing the best predictive variables by an automatic procedure
according to a selected model criterion (Akaike information criterion (AIC), Bayesian information criterion (BIC)).
We define the sets of variables indices <img class="math" src="../../_images/math/c588515c3ce99e25b54ad64068ebaa4c01da4894.svg" alt="S_{min}\,\subseteq\, S_0\,\subseteq\, S_{max}"/>.
Let us consider <img class="math" src="../../_images/math/d5e0954e2c17461974843f24ca4fe4e23b466c00.svg" alt="S"/> a set of variables indices and <img class="math" src="../../_images/math/f41d978c0bbf0c8143a8ae201ca760ee315f9a9e.svg" alt="\# S"/> the cardinal of <img class="math" src="../../_images/math/d5e0954e2c17461974843f24ca4fe4e23b466c00.svg" alt="S"/>, the (AIC) and (BIC) criteria are:</p>
<ul class="simple">
<li>(AIC): <img class="math" src="../../_images/math/8ac5c6ea58690633b7fe315dab435f617c2d9fc1.svg" alt="-2\,\log L(\hat{\beta},\hat{\sigma}\mid Y) + 2 \times \# S"/></li>
<li>(BIC): <img class="math" src="../../_images/math/38d7a1dc4bdb09c85e1d2fe7803ed32a0defca1a.svg" alt="-2\,\log L(\hat{\beta},\hat{\sigma}\mid Y) + \log(n) \times \# S"/>.</li>
</ul>
<p>Using equation <a class="reference internal" href="#equation-maxloglikelihood">(2)</a> we get:
<img class="math" src="../../_images/math/66a7532fc25708a86afcb820bc70b0312cf7d3e7.svg" alt="(AIC) \,:\,  n\, \log(\hat{\sigma}^2) + C_1 + 2 \times \# S"/> and
<img class="math" src="../../_images/math/c9d954b1b057cef3ac3b759f5b468fbf3ba0f2d6.svg" alt="(BIC) \,:\,  n\, \log(\hat{\sigma}^2) + C_1 +\log(n) \times \# S"/>,
where the constant <img class="math" src="../../_images/math/91744dab39a363d9b99f2600a53ee3560aee74ee.svg" alt="C_1"/> is defined by <img class="math" src="../../_images/math/4a0c00541f2498a851f917836459fa785515a526.svg" alt="C_1 = n \big(\log(2\pi)+1\big)"/>.
However for model comparisons, only differences in AIC (or BIC) criterion are meaningful,
consequently the constant <img class="math" src="../../_images/math/91744dab39a363d9b99f2600a53ee3560aee74ee.svg" alt="C_1"/> can be ignored, which conveniently allows us to
take as in R <code class="docutils literal notranslate"><span class="pre">step</span></code> method:</p>
<div class="math">
<p><img src="../../_images/math/1cdc625c78e8be27b26f9a9652a149c9fa8db239.svg" alt="AIC &amp;:  n\, \log(\hat{\sigma}^2) + 2 \times \# S  \\
BIC &amp;:  n\, \log(\hat{\sigma}^2) +\log(n) \times \# S"/></p>
</div><p>There are three different algorithms: by using forward selection, backward selection, or both.</p>
<div class="section" id="forward-selection">
<h4>Forward selection<a class="headerlink" href="#forward-selection" title="Permalink to this headline">¶</a></h4>
<p>This method starts with initial variables in the model (defined by the set of indices <img class="math" src="../../_images/math/d2fe8547a74da801e66cd7ce91994c10590da04c.svg" alt="S_0"/>), testing the addition of each variable
using a chosen model comparison criterion, adding the variable (if any) that improves the model the most, and repeating this process until none improves the model.
We define <img class="math" src="../../_images/math/2ffc46e874327a6a7ebd51bbe6185d3d42502873.svg" alt="X_{+i}"/> the <img class="math" src="../../_images/math/2e15966e9234993e24b6547cdf18beb64ab20e6f.svg" alt="(n \times (p+1))"/> matrix composed by <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/> matrix and <img class="math" src="../../_images/math/720e690c8e7097293116ec591e6077ec1f347afc.svg" alt="x_i"/> column: <img class="math" src="../../_images/math/7bfcfd2164399080c6d878fc1ff27485368b0459.svg" alt="X_{+i} = (X \,,\,x_i)"/>.
We define <img class="math" src="../../_images/math/cab5e7607d560ea6c165f4ae79584992a29c6a01.svg" alt="\hat{\beta}_{+i}"/> the vector of size <img class="math" src="../../_images/math/495d0493108ac4b336ef86a0475d77cb7160fcf3.svg" alt="(p+1)"/> and the scalar <img class="math" src="../../_images/math/4c473f5c315c5b12e52e06c15125ad902adcc7b1.svg" alt="\hat{\sigma}_{+i}^2"/> by:</p>
<div class="math">
<p><img src="../../_images/math/5e794c1e1f54d3c9d99095aeb526b97e9af841b4.svg" alt="\hat{\beta}_{+i} \,=\, \big(X^T_{+i} \,X_{+i}\big)^{-1} \,X^T_{+i} \, Y
\quad,\quad
\hat{\sigma}_{+i}^2 = \frac{1}{n}\big(Y-X_{+i} \,\hat{\beta}_{+i}\big)^T\,\big(Y-X_{+i} \,\hat{\beta}_{+i}\big)"/></p>
</div><p>We define <img class="math" src="../../_images/math/f344a0d60b69c9245c9f7245317e52fa411fbe35.svg" alt="H_{+i}"/> the <img class="math" src="../../_images/math/21effe8463dfdca7fe64474fc05c8f672f8a8dc3.svg" alt="(n\times n)"/> projection matrix by:</p>
<div class="math" id="equation-hp">
<p><span class="eqno">(3)<a class="headerlink" href="#equation-hp" title="Permalink to this equation">¶</a></span><img src="../../_images/math/5034ab528801f6e1adeef235a15daca2dc1a559a.svg" alt="H_{+i}\, \,\hat{=} \, X_{+i}\,\big(X^T_{+i} \,X_{+i}\big)^{-1} \,X^T_{+i}"/></p>
</div><p>The Forward selection algorithm looks like this:</p>
<ol class="arabic simple">
<li>Input: <img class="math" src="../../_images/math/d2fe8547a74da801e66cd7ce91994c10590da04c.svg" alt="S_0"/>, <img class="math" src="../../_images/math/f485fc0c7b5b2f36b76d9dab96a924e13fab1a68.svg" alt="S_{max}"/>,  <img class="math" src="../../_images/math/6a021e5a5c0737edb1db932d58fb1b110239d3c2.svg" alt="\mbox{\ttfamily penalty\_} = \{2,\log(n)\}"/>,  <img class="math" src="../../_images/math/1ce9cb8429b61324e8fb0bad8fe67b135bd775af.svg" alt="\mbox{\ttfamily maxIter\_}"/></li>
<li>Initialization: <img class="math" src="../../_images/math/882f2a9af8ddcd2105ed166a0767c4f88fe0e9aa.svg" alt="S^* = S_0"/>, <img class="math" src="../../_images/math/20621134530d4d0bd0273a31d14ea3ef55b2fa1d.svg" alt="n_{iter} = 0"/></li>
<li>We compute <img class="math" src="../../_images/math/895dea5ebc41d7ee58d5d769c74ef17efc4d9622.svg" alt="J^* = \log L(\hat{\beta},\hat{\sigma}\mid Y)"/></li>
<li>While (<img class="math" src="../../_images/math/e1dca209b706bbcbc409aa15575b939b2905d63e.svg" alt="n_{iter} &lt; \mbox{\ttfamily maxIter\_}"/>)<ol class="loweralpha">
<li>We compute <img class="math" src="../../_images/math/dd00cc9084fb52b72d1aeee91204a8b93402a2d5.svg" alt="J^i = \log L(\hat{\beta}_{+i},\hat{\sigma}_{+i}\mid Y)"/>
where <img class="math" src="../../_images/math/3f9b77e5b4dae914a919ef99865f6a00351ea0f8.svg" alt="\boxed{\,i = \displaystyle\arg \max_{j \in S_{max} \backslash S^*}\,\log L(\hat{\beta}_{+j},\hat{\sigma}_{+j}\mid Y) \,}"/></li>
<li>if (<img class="math" src="../../_images/math/eced66d3d5584620f2a1d1d5386394c9c9e1b059.svg" alt="(J^i+\mbox{\ttfamily penalty\_} &lt; J^*)"/>), then set <img class="math" src="../../_images/math/347c31500ca45ebaffd451349dae0b91876d26df.svg" alt="S^* =S^* \, \cup\, i"/> and
<img class="math" src="../../_images/math/a4faa96498aaa16e8b97a83fb3b0b968098cdc2c.svg" alt="J^* = J^i"/></li>
<li>else quit</li>
<li><img class="math" src="../../_images/math/3b1151334a15a2d777da53706c1d1cc8bf9bc551.svg" alt="n_{iter} = n_{iter} + 1"/></li>
</ol>
</li>
</ol>
<p>Note that using equation <a class="reference internal" href="#equation-maxloglikelihood">(2)</a>, we have:</p>
<div class="math">
<p><img src="../../_images/math/b12a9cd61503c9e1c1354dc554b7a3ca5a921959.svg" alt="\arg \displaystyle\max_{j \in S_{max} \backslash S^*}\,  \log L(\hat{\beta}_{+j},\hat{\sigma}_{+j}\mid Y) =
\arg \displaystyle\min_{j \in S_{max} \backslash S^*}\, \|\,Y-H_{+j}\,Y\,\|^2_2  \,\,"/></p>
</div><p>Consequently to find the best variable to add we can consider the least square of the residual term <img class="math" src="../../_images/math/5e34b4a8831c3420f318e9fe204f28449d7528c3.svg" alt="Y-H_{+i}\,Y"/>.</p>
</div>
<div class="section" id="backward-selection">
<h4>Backward selection<a class="headerlink" href="#backward-selection" title="Permalink to this headline">¶</a></h4>
<p>This method starts with all candidate variables
(defined by the set of indices <img class="math" src="../../_images/math/f485fc0c7b5b2f36b76d9dab96a924e13fab1a68.svg" alt="S_{max}"/>), testing the deletion of each variable using a chosen model comparison criterion,
deleting the variable (if any) that improves the model the most by being deleted, and repeating this process until no further improvement is possible.
We define <img class="math" src="../../_images/math/17f1f6264c8fe50fdb5e1fe98403b4df717924e8.svg" alt="X_{-i}"/> the <img class="math" src="../../_images/math/fb0c8712cc1ba1d1a96815fdc928e98313c6a585.svg" alt="(n \times (p-1))"/> matrix composed by <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/> matrix without the <img class="math" src="../../_images/math/720e690c8e7097293116ec591e6077ec1f347afc.svg" alt="x_i"/> column.
We define <img class="math" src="../../_images/math/56680cb2d2dd41a8019ae099ae0cd298dba7bbf6.svg" alt="\hat{\beta}_{-i}"/> the vector of size <img class="math" src="../../_images/math/a8166e4a45546f0070ff201676f40d10a57475da.svg" alt="(p-1)"/> and the scalar <img class="math" src="../../_images/math/993e03e0817a4c6587096ffb26f709fea6a28f9d.svg" alt="\hat{\sigma}_{-i}^2"/> by:</p>
<div class="math">
<p><img src="../../_images/math/4624c16d981cba8eaca55f0692fc0ddb9a1407b4.svg" alt="\hat{\beta}_{-i} \,=\, \big(X^T_{-i} \,X_{-i}\big)^{-1} \,X^T_{-i} \, Y
\quad,\quad
\hat{\sigma}_{-i}^2 = \frac{1}{n}\big(Y-X_{-i} \,\hat{\beta}_{-i}\big)^T\,\big(Y-X_{-i} \,\hat{\beta}_{-i}\big)"/></p>
</div><p>We define <img class="math" src="../../_images/math/7d0d75c4377e4615fa110649156a349bf5e5c093.svg" alt="H_{-i}"/> the <img class="math" src="../../_images/math/21effe8463dfdca7fe64474fc05c8f672f8a8dc3.svg" alt="(n\times n)"/> projection matrix by:</p>
<div class="math" id="equation-hm">
<p><span class="eqno">(4)<a class="headerlink" href="#equation-hm" title="Permalink to this equation">¶</a></span><img src="../../_images/math/39ace23aca349b7a1288869edde951ffa38f2ce2.svg" alt="H_{-i}\, \,\hat{=}\, X_{-i}\,\big(X^T_{-i} \,X_{-i}\big)^{-1} \,X^T_{-i}"/></p>
</div><p>The Backward selection algorithm looks like this:</p>
<ol class="arabic simple">
<li>Input: <img class="math" src="../../_images/math/d2fe8547a74da801e66cd7ce91994c10590da04c.svg" alt="S_0"/>, <img class="math" src="../../_images/math/327ebab5764b7b25928c7c3c021d4b1245205e17.svg" alt="S_{min}"/>,  <img class="math" src="../../_images/math/6a021e5a5c0737edb1db932d58fb1b110239d3c2.svg" alt="\mbox{\ttfamily penalty\_} = \{2,\log(n)\}"/>,  <img class="math" src="../../_images/math/1ce9cb8429b61324e8fb0bad8fe67b135bd775af.svg" alt="\mbox{\ttfamily maxIter\_}"/></li>
<li>Initialization: <img class="math" src="../../_images/math/882f2a9af8ddcd2105ed166a0767c4f88fe0e9aa.svg" alt="S^* = S_0"/>, <img class="math" src="../../_images/math/20621134530d4d0bd0273a31d14ea3ef55b2fa1d.svg" alt="n_{iter} = 0"/>
We compute <img class="math" src="../../_images/math/895dea5ebc41d7ee58d5d769c74ef17efc4d9622.svg" alt="J^* = \log L(\hat{\beta},\hat{\sigma}\mid Y)"/></li>
<li>While (<img class="math" src="../../_images/math/ff6eee39ad92bd17333c4c64e4e32632a11c5ab9.svg" alt="n_{iter} &lt;  \mbox{\ttfamily maxIter\_}"/>)<ol class="loweralpha">
<li>We compute <img class="math" src="../../_images/math/8cb4d1b66c6acb99288ebeed4b656ba0711d6286.svg" alt="J^i = \log L(\hat{\beta}_{-i},\hat{\sigma}_{-i}\mid Y)"/>
where <img class="math" src="../../_images/math/39e1bdbd4fa06e11ecb99f1b955667c9fb8497e9.svg" alt="\boxed{\,i = \displaystyle\arg \max_{j \in S^*\backslash S_{min}}\,\log L(\hat{\beta}_{-j},\hat{\sigma}_{-j}\mid Y) \,}"/></li>
<li>if (<img class="math" src="../../_images/math/09c34c147c4e7a483a731f6976d3b53731a5fbe6.svg" alt="(J^i-\mbox{\ttfamily penalty\_} &lt; J^*)"/>), then set
<img class="math" src="../../_images/math/775960e2d8683f195dc8c9aa43c11b4d6a9c5e17.svg" alt="S^* =S^* \, \backslash\,  i"/> and <img class="math" src="../../_images/math/a4faa96498aaa16e8b97a83fb3b0b968098cdc2c.svg" alt="J^* = J^i"/></li>
<li>else quit</li>
<li><img class="math" src="../../_images/math/3b1151334a15a2d777da53706c1d1cc8bf9bc551.svg" alt="n_{iter} = n_{iter} + 1"/></li>
</ol>
</li>
</ol>
<p>Using equation <a class="reference internal" href="#equation-maxloglikelihood">(2)</a>, we have:</p>
<div class="math">
<p><img src="../../_images/math/707f7c5260942a4ed289a4e7c23b8f8f340d150d.svg" alt="\arg   \displaystyle\max_{j \in S^*\backslash S_{min}}\,  \log L(\hat{\beta}_{-j},\hat{\sigma}_{-j}\mid Y) =
\arg \displaystyle\min_{j \in S^*\backslash S_{min}}\, \|\,Y-H_{-j}\,Y\,\|^2_2"/></p>
</div><p>Consequently to find the best variable to delete we can consider the least square of the residual term <img class="math" src="../../_images/math/525612352f41f9534bbc6a9234de029b5e1589be.svg" alt="Y-H_{-i}\,Y"/>.</p>
</div>
<div class="section" id="bidirectional-selection">
<h4>Bidirectional selection<a class="headerlink" href="#bidirectional-selection" title="Permalink to this headline">¶</a></h4>
<p>This method is a combination of the Forward and Backward selection. At each step, this method tests
the addition (Forward selection) and the deletion (Backward selection) of each variable using a chosen model comparison criterion,
select the method that improves the model the most, and repeat this process.</p>
<p>The Bidirectional selection algorithm is the following:</p>
<ol class="arabic simple">
<li>Input: <img class="math" src="../../_images/math/d2fe8547a74da801e66cd7ce91994c10590da04c.svg" alt="S_0"/>, <img class="math" src="../../_images/math/327ebab5764b7b25928c7c3c021d4b1245205e17.svg" alt="S_{min}"/>, <img class="math" src="../../_images/math/f485fc0c7b5b2f36b76d9dab96a924e13fab1a68.svg" alt="S_{max}"/>, <img class="math" src="../../_images/math/6a021e5a5c0737edb1db932d58fb1b110239d3c2.svg" alt="\mbox{\ttfamily penalty\_} = \{2,\log(n)\}"/>,  <img class="math" src="../../_images/math/1ce9cb8429b61324e8fb0bad8fe67b135bd775af.svg" alt="\mbox{\ttfamily maxIter\_}"/></li>
<li>Initialization: <img class="math" src="../../_images/math/882f2a9af8ddcd2105ed166a0767c4f88fe0e9aa.svg" alt="S^* = S_0"/>, <img class="math" src="../../_images/math/20621134530d4d0bd0273a31d14ea3ef55b2fa1d.svg" alt="n_{iter} = 0"/>
We compute <img class="math" src="../../_images/math/895dea5ebc41d7ee58d5d769c74ef17efc4d9622.svg" alt="J^* = \log L(\hat{\beta},\hat{\sigma}\mid Y)"/></li>
<li>While (<img class="math" src="../../_images/math/ff6eee39ad92bd17333c4c64e4e32632a11c5ab9.svg" alt="n_{iter} &lt;  \mbox{\ttfamily maxIter\_}"/>)<ol class="loweralpha">
<li>We compute <img class="math" src="../../_images/math/dd00cc9084fb52b72d1aeee91204a8b93402a2d5.svg" alt="J^i = \log L(\hat{\beta}_{+i},\hat{\sigma}_{+i}\mid Y)"/>
where <img class="math" src="../../_images/math/3f9b77e5b4dae914a919ef99865f6a00351ea0f8.svg" alt="\boxed{\,i = \displaystyle\arg \max_{j \in S_{max} \backslash S^*}\,\log L(\hat{\beta}_{+j},\hat{\sigma}_{+j}\mid Y) \,}"/></li>
<li>We compute <img class="math" src="../../_images/math/dfe5a9d0241e25b2b9e9cb35ed041fe55665199f.svg" alt="J^{i'} = \log L(\hat{\beta}_{-i},\hat{\sigma}_{-i}\mid Y)"/>
where <img class="math" src="../../_images/math/415151732b3cb30e4d5dbb25dddff24602eb454d.svg" alt="\boxed{\,i' = \displaystyle\arg \max_{j \in S^*\backslash S_{min}}\,\log L(\hat{\beta}_{-j},\hat{\sigma}_{-j}\mid Y) \,}"/></li>
<li>if (<img class="math" src="../../_images/math/eced66d3d5584620f2a1d1d5386394c9c9e1b059.svg" alt="(J^i+\mbox{\ttfamily penalty\_} &lt; J^*)"/>) or (<img class="math" src="../../_images/math/a18c31a54ce8a955a3dfb440a0e76679807351f4.svg" alt="(J^{i'}-\mbox{\ttfamily penalty\_} &lt; J^*)"/>),<ol class="lowerroman">
<li>if (<img class="math" src="../../_images/math/eced66d3d5584620f2a1d1d5386394c9c9e1b059.svg" alt="(J^i+\mbox{\ttfamily penalty\_} &lt; J^*)"/>), set <img class="math" src="../../_images/math/7e420a3f58e0b7963747c6cebea943b87bdf51de.svg" alt="S^* =S^* \,\cup \,  i"/> and <img class="math" src="../../_images/math/103b015abea5e5e9b5101e05add728b8c8f52551.svg" alt="J^* = J^{i}"/></li>
<li>else set <img class="math" src="../../_images/math/353982219d35c14a6d81a1b95d1636f6112eb5ff.svg" alt="S^* =S^* \,\backslash \,  i'"/> and <img class="math" src="../../_images/math/d8710b30b7e27826c328d93fabf55c5b89edf6af.svg" alt="J^* = J^{i'}"/></li>
</ol>
</li>
<li>else quit</li>
<li><img class="math" src="../../_images/math/3b1151334a15a2d777da53706c1d1cc8bf9bc551.svg" alt="n_{iter} = n_{iter} + 1"/></li>
</ol>
</li>
</ol>
<p>Using equation <a class="reference internal" href="#equation-maxloglikelihood">(2)</a>, we have:</p>
<div class="math">
<p><img src="../../_images/math/c4c54b79068910fdcbce1e8dca34dbd7c462e22e.svg" alt="\arg\displaystyle\max_{j \in S^*\backslash S_{min}}\,  \log L(\hat{\beta}_{-j},\hat{\sigma}_{-j}\mid Y) &amp;=
\arg\displaystyle\min_{j \in S^*\backslash S_{min}}\, \|\,Y-H_{-j}\,Y\,\|^2_2  \\
\arg\displaystyle\max_{j \in S_{max} \backslash S^*}\,  \log L(\hat{\beta}_{+j},\hat{\sigma}_{+j}\mid Y)  &amp;=
\arg\displaystyle\min_{j \in S_{max} \backslash S^*}\, \|\,Y-H_{+j}\,Y\,\|^2_2"/></p>
</div><p>Consequently to find the best variable to add (resp. to delete), we can consider the least square of the residual term <img class="math" src="../../_images/math/92f6494cf8a535d067bc9aefe2b097f4c066dfb8.svg" alt=":Y-H_{+i}\,Y"/>
(resp.  <img class="math" src="../../_images/math/76c3160ee870f5dd052e38df554f959f51cca237.svg" alt=":Y-H_{-i}\,Y"/>).</p>
<div class="figure align-center" id="id5">
<img alt="LinearModelStepwiseAlgorithm class" src="../../_images/LinearModelStepwiseAlgorithm.png" />
<p class="caption"><span class="caption-text">LinearModelStepwiseAlgorithm class</span></p>
</div>
</div>
</div>
<div class="section" id="detailed-implementation">
<h3>Detailed implementation<a class="headerlink" href="#detailed-implementation" title="Permalink to this headline">¶</a></h3>
<p>Each selection method requires to find an index which minimizes some residual norm.
In this section, we explain how computations can be performed very efficiently, by
minimizing the number of operations.</p>
<div class="section" id="qr-decomposition-of-matrix-x">
<h4>QR decomposition of matrix <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/><a class="headerlink" href="#qr-decomposition-of-matrix-x" title="Permalink to this headline">¶</a></h4>
<p>Note that in practice <img class="math" src="../../_images/math/7843c4327ea77abbd4f9f260615d686e5f4f6a3e.svg" alt="n &gt;&gt; p"/> and consequently we do not want to compute <img class="math" src="../../_images/math/10dcf85389521ab8928b45b595e17525c54c4127.svg" alt="H_X"/>
the projection matrix of size <img class="math" src="../../_images/math/21effe8463dfdca7fe64474fc05c8f672f8a8dc3.svg" alt="(n\times n)"/>.
We also do not have to compute <img class="math" src="../../_images/math/76531441490dd4e4f665e244fe9eff6ba9406442.svg" alt="A_X"/> the inverse Gram matrix of <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/> of size <img class="math" src="../../_images/math/a788dede5b50f9ae1e949b74501b9c1bc0a4f783.svg" alt="(p\times p)"/>, all we need
is to solve linear systems.
Since matrix <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/> has full rank, it can be written as a product <img class="math" src="../../_images/math/e5e0f56bf36154f70dcca469ede144107ca06e9c.svg" alt="X = Q_X\,R_X"/>
of a matrix <img class="math" src="../../_images/math/1af16b202b92e68b24f79b8bfe037819e282bb24.svg" alt="Q_X"/> of size <img class="math" src="../../_images/math/21796bef7046b998e5e5889d2713bbe1c38e2f84.svg" alt="(n\times p)"/> having orthogonal columns and an upper triangular matrix <img class="math" src="../../_images/math/5a8487fe4cb61dbeee5c09b906630c6a66663ab4.svg" alt="R_X"/>
of size <img class="math" src="../../_images/math/a788dede5b50f9ae1e949b74501b9c1bc0a4f783.svg" alt="(p\times p)"/> with positive diagonal matrices.</p>
<p>Using the thin QR decomposition of matrix <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/> we obtain:</p>
<div class="math">
<p><img src="../../_images/math/48b8d8adde6bbac500cac8d2299463e3f2a8ef59.svg" alt="A_X &amp;= \big(X^T\,X\big)^{-1} = \big(R_X^T\,Q_X^T\,Q_X\,R_X\big)^{-1} = \big(R_X^T\,R_X\big)^{-1}= R_X^{-1}\,R_X^{-T} \\
H_X &amp;= X\,\big(X^T\,X\big)^{-1}\,X^T  = X\,A_X\,X^T = Q_X\,R_X\,R_X^{-1}\,R_X^{-T}\,R_X^T\,Q_X^T =Q_X\, Q_X^T"/></p>
</div></div>
<div class="section" id="id1">
<h4>Forward selection<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>It can be shown that the inverse Gram matrix of <img class="math" src="../../_images/math/2ffc46e874327a6a7ebd51bbe6185d3d42502873.svg" alt="X_{+i}"/> of size <img class="math" src="../../_images/math/d342b42a3a677e2cda373274cdb5c28d01120caa.svg" alt="((p+1)\times(p+1))"/>  can be represented by a block partition</p>
<div class="math">
<p><img src="../../_images/math/510194647d4d16eb2cb26967ab0d4359159452e2.svg" alt="\big(X^T_{+i} \,X_{+i}\big)^{-1} =
 \begin{bmatrix}
A_X + D_X\,D_X^T/C_X  &amp; -D_X/C_X \\
D_X^T/C_X &amp; 1/C_X
\end{bmatrix}
 \quad,\quad D_X = A_X\, X^T\,x_i
 \quad,\quad C_X = x_i^T x_i -x_i^T \,X\,A \, X^T\, x_i"/></p>
</div><p>Then the projection matrix <img class="math" src="../../_images/math/f344a0d60b69c9245c9f7245317e52fa411fbe35.svg" alt="H_{+i}"/> defined by equation <a class="reference internal" href="#equation-hp">(3)</a> turns into:</p>
<div class="math">
<p><img src="../../_images/math/ddb53650139ae1314bb8243b2cef15fd4b8d3ff8.svg" alt="H_{+i} &amp; = X\,A_X \, X^T + \frac{1}{C_X} \big(\,X\,A_X \, X^T\,x_i\,x_i^T\,X\,A_X \, X^T \,-\,X\,A_X \, X^T\,x_i\,x_i^T \,-\,x_i\,x_i^T \, X\,A_X \, X^T\,+\,x_i\,x_i^T \,\big)"/></p>
</div><p>We get the residual term:</p>
<div class="math" id="equation-defhpy">
<p><span class="eqno">(5)<a class="headerlink" href="#equation-defhpy" title="Permalink to this equation">¶</a></span><img src="../../_images/math/c943e701cf4df19023c838ee2a737d386114970b.svg" alt="Y-H_{+i}\,Y  &amp; = Y-X\,A_X \, X^T\,Y -\frac{(x_i^T\,X\,A_X \, X^T\,Y-x_i^T\,Y)}{C_X}\, \big(\,X\,A_X \, X^T\,x_i\, \,-\,x_i\,\big)\\
 &amp; = Y - H_X\,Y -\frac{x_i^T\,(Y\,-\,H_X\,Y)}{x_i^T\,(H_X\,x_i\, \,-\,x_i)}\, \big(\,H_X\,x_i\, \,-\,x_i\,\big)\\
 &amp; = Y - \hat{Y} -\frac{x_i^T\,(Y\,-\,\hat{Y})}{x_i^T\,(H_X\,x_i\, \,-\,x_i)}\, \big(\,H_X\,x_i\, \,-\,x_i\,\big)"/></p>
</div></div>
<div class="section" id="implementation-using-qr-decomposition">
<h4>Implementation using QR decomposition<a class="headerlink" href="#implementation-using-qr-decomposition" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>The vector <img class="math" src="../../_images/math/31398cc2c1d11f16ed6bb118ea5e48820f368a6b.svg" alt="\hat{Y}=H_X\,Y=Q_X\,Q_X^T\,Y"/> of size <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> does not depend on the column <img class="math" src="../../_images/math/720e690c8e7097293116ec591e6077ec1f347afc.svg" alt="x_i"/> to add.
The computation of this vector is done by two matrix-vector products.</li>
<li>The vector <img class="math" src="../../_images/math/0827918fe6a95555622c871471c5ec7a9e1afd9f.svg" alt="\,H_X\,x_i\,= Q_X\,Q_X^T\,x_i"/> of size <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> depends on the column <img class="math" src="../../_images/math/720e690c8e7097293116ec591e6077ec1f347afc.svg" alt="x_i"/> to add.
The computation of this vector is done by two matrix-vector products.</li>
</ul>
</div>
<div class="section" id="id2">
<h4>Backward selection<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>The projection matrix <img class="math" src="../../_images/math/7d0d75c4377e4615fa110649156a349bf5e5c093.svg" alt="H_{-i}"/> defined by equation (ref{Hm}) turns into:</p>
<div class="math" id="equation-h2m">
<p><span class="eqno">(6)<a class="headerlink" href="#equation-h2m" title="Permalink to this equation">¶</a></span><img src="../../_images/math/c61c21f130e9df79ab937eefe5cbb35e7951e83c.svg" alt="H_{-i}\, \,\hat{=}\, X_{-i}\,\big(X^T_{-i} \,X_{-i}\big)^{-1} \,X^T_{-i}
   = X_{-i}\,A_{-i,-i} \, X^T_{-i} \,-\,\frac {1}{A_{i,i}}\,\big(X_{-i}\, A_{-i,i}\big)\, \big(X_{-i}\, A_{-i,i}\big)^T"/></p>
</div><p>where <img class="math" src="../../_images/math/5e774b1fd40ebca33820e501a685dd41e2e0eb80.svg" alt="A_{-i,-i}"/> represents the matrix <img class="math" src="../../_images/math/91304637f2db0cf034ab1ed21ce1701dde088e86.svg" alt="A"/> without row <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/> and column <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/>,
<img class="math" src="../../_images/math/afc5a1b2b81956d137865e25106cd0d1f4052dca.svg" alt="A_{-i,i}"/> represents the column <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/> of the matrix <img class="math" src="../../_images/math/91304637f2db0cf034ab1ed21ce1701dde088e86.svg" alt="A"/> without row <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/> and <img class="math" src="../../_images/math/418528a528dcbb499aa6f91ad943593e768ad7a9.svg" alt="A_{i,i}"/> represents the diagonal term <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/> of the matrix <img class="math" src="../../_images/math/91304637f2db0cf034ab1ed21ce1701dde088e86.svg" alt="A"/>.</p>
<p>In order to avoid matrix copies, we want to use the matrix <img class="math" src="../../_images/math/76531441490dd4e4f665e244fe9eff6ba9406442.svg" alt="A_X"/> in the equation
<a class="reference internal" href="#equation-h2m">(6)</a> without creating matrices <img class="math" src="../../_images/math/f03be5fd30c5d6c925a132c214a049408a590851.svg" alt="A_{-i }"/>.
To this end, we define <img class="math" src="../../_images/math/3386f2ba97f132e5d112dc3f6fd0cb2c4e5b0f85.svg" alt="X_{i=0}"/> a matrix <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/> whose column <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/> contains only <img class="math" src="../../_images/math/f5290354bef6fef6abea62d61ee6640a5d8acda5.svg" alt="0"/>,
and <img class="math" src="../../_images/math/56f2e473c8ad8c77b911035bb451d93a1865b47e.svg" alt="\forall B \in \mathbb{R}^p"/> we note <img class="math" src="../../_images/math/39bb25bc5c66f545d1bf021151d242c78a904f26.svg" alt="\big[B\big]_{i=0}"/> a copy of <img class="math" src="../../_images/math/a13a49f873508441188fc44369247be63c66c9a4.svg" alt="B"/> which has
its <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/>-th row equals to <img class="math" src="../../_images/math/f5290354bef6fef6abea62d61ee6640a5d8acda5.svg" alt="0"/>.
We get: <img class="math" src="../../_images/math/a7ecc1aeab4330d7916a7bfe3ac94cdb66fa8b66.svg" alt="\forall b \in \mathbb{R}^n\,,\, \forall c \in \mathbb{R}^p"/></p>
<div class="math" id="equation-notation0">
<p><span class="eqno">(7)<a class="headerlink" href="#equation-notation0" title="Permalink to this equation">¶</a></span><img src="../../_images/math/7cfa44fa1aa504162b21aa18d24e3c6f25aaf0f1.svg" alt="X_{i=0}^T\,b \,=\,\big[X^T\,b\big]_{i=0} \quad,\quad X_{i=0}\,c \,=\,X\,\big[c\big]_{i=0}"/></p>
</div><p>Using equation <a class="reference internal" href="#equation-notation0">(7)</a>, the projection matrix <img class="math" src="../../_images/math/7d0d75c4377e4615fa110649156a349bf5e5c093.svg" alt="H_{-i}"/> defined by equation <a class="reference internal" href="#equation-hm">(4)</a> turns into:</p>
<div class="math">
<p><img src="../../_images/math/2ce0d31d7fa86624e62eb685bdd9dc354b68641c.svg" alt="H_{-i}\, &amp; = X_{i=0}\,A_X\,X_{i=0}^T \,-\,\frac {1}{A_{i,i}}\,  \big(X_{i=0}\,A_{,i}\big) \big(X_{i=0}\,A_{,i}\big)^T   \\
&amp; = X_{i=0}\,A_X\,X_{i=0}^T \,-\,\frac {1}{A_{i,i}}\,  \big(X\,\big[A_{,i}\big]_{i=0}  \big) \big(X\,\big[A_{,i}\big]_{i=0} \big)^T"/></p>
</div><p>Using equation <a class="reference internal" href="#equation-notation0">(7)</a>, we get the residual term:</p>
<div class="math" id="equation-defhmy">
<p><span class="eqno">(8)<a class="headerlink" href="#equation-defhmy" title="Permalink to this equation">¶</a></span><img src="../../_images/math/c0cf3ebd1bcff3a4773bfa757fad0c091130ca91.svg" alt="Y-H_{-i}\,Y &amp; = Y-\,X_{i=0}\,A_X\,X_{i=0}^T\,Y \,+\,\frac {1}{A_{i,i}}\,  \big(X_{i=0}\,A_{,i}\,(A_{i,} X_{i=0}^T\,Y)\,\big)\\
            &amp; = Y-\,X\,\big[\,A_X\,\big[X^T\,Y\big]_{i=0}\,\big]_{i=0} \,+\,\frac {1}{A_{i,i}}\,  \big( X\,\big[\,A_{,i}\,\big]_{i=0}\,\,(A_{i,} \,\big[X^T\,Y\big]_{i=0})\,\big)\\
            &amp; = Y-\,X\,\big[\,A_X\,\big[X^T\,Y\big]_{i=0}\, -\,\frac {A_{i,} \,\big[X^T\,Y\big]_{i=0}}{A_{i,i}}\,A_{,i}\,\big]_{i=0}\\
            &amp; = Y-\,X\,\big(\,A_X\,\big[X^T\,Y\big]_{i=0}\, -\,\frac {A_{i,} \,\big[X^T\,Y\big]_{i=0}}{A_{i,i}}\,A_{,i}\,\big)"/></p>
</div><p>Then we rewrite the residual term equation <a class="reference internal" href="#equation-defhmy">(8)</a> using <img class="math" src="../../_images/math/4598ec1ae5d311fbec8a306b78378c8e14426f4d.svg" alt="e_i"/> the vector of size <img class="math" src="../../_images/math/2640e9252aac1782adab24797b02ae248a680467.svg" alt="(p)"/> with a <img class="math" src="../../_images/math/cb6b1988abf53551b8d2e6f27183f7d8478fe0b4.svg" alt="1"/> in the <img class="math" src="../../_images/math/360c07ef89da0fcb8a26f2a073124f0cad6ae65c.svg" alt="i^{th}"/> coordinates and <img class="math" src="../../_images/math/f5290354bef6fef6abea62d61ee6640a5d8acda5.svg" alt="0"/> elsewhere.
We obtain:</p>
<div class="math" id="equation-defhmy2">
<p><span class="eqno">(9)<a class="headerlink" href="#equation-defhmy2" title="Permalink to this equation">¶</a></span><img src="../../_images/math/be6f76a4c497d9a7f2720f6598fe007c5329da8f.svg" alt="Y-H_{-i}\,Y &amp; = Y- \,X\,\big(\,A_X\,(X^T\,Y-x_i^T\,Y\,e_i)\, -\,\frac {(A_X\,e_i)^T \,(X^T\,Y-x_i^T\,Y\,e_i)}{A_{i,i}}\,A_X\,e_i\,\big)  \\
            &amp; = Y- \,X\, A_X\,X^T\,Y \,+\, (x_i^T\,Y)\,X\, A_X\,e_i \,+\,\frac { e_i^T\,A_X\,X^T\,Y-(x_i^T\,Y) \,e_i^T\,A_X\,e_i}{A_{i,i}} \,X\, A_X\,e_i  \\
            &amp; = Y- \hat{Y} \,+\, (x_i^T\,Y)\,X\, A_X\,e_i \,+\,\frac { e_i^T\,A_X\,X^T\,Y}{A_{i,i}} \,X\, A_X\,e_i -(x_i^T\,Y)\,X\, A_X\,e_i \\
            &amp; = Y- \hat{Y} \,+\,\,\frac { (X\, A_X\,e_i)^T\,Y}{A_{i,i}} \,X\, A_X\,e_i"/></p>
</div></div>
<div class="section" id="id3">
<h4>Implementation using QR decomposition<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>The vector <img class="math" src="../../_images/math/31398cc2c1d11f16ed6bb118ea5e48820f368a6b.svg" alt="\hat{Y}=H_X\,Y=Q_X\,Q_X^T\,Y"/> of size <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> does not depend on the column <img class="math" src="../../_images/math/720e690c8e7097293116ec591e6077ec1f347afc.svg" alt="x_i"/> to delete.
The computation of this vector is done by two matrix-vector products.</li>
<li>The vector <img class="math" src="../../_images/math/92a96626efd9beb4f2583b7b1bf333ebe400e7c5.svg" alt="\,X\, A_X\,e_i=Q_X\,R_X\,R_X^{-1}\,R_X^{-T}\,e_i = Q_X\,R_X^{-T}\,e_i"/> of size <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> depends on <img class="math" src="../../_images/math/720e690c8e7097293116ec591e6077ec1f347afc.svg" alt="x_i"/> the column to delete.
The computation of this vector is done by two matrix-vector products:<ol class="arabic">
<li>First we compute the vector of size <img class="math" src="../../_images/math/2640e9252aac1782adab24797b02ae248a680467.svg" alt="(p)"/>: <img class="math" src="../../_images/math/faf685f4f85f7bc3b6745a83b681518016cb91ea.svg" alt="b_i=R_X^{-T}\,e_i"/>.</li>
<li>Then we compute the vector of size <img class="math" src="../../_images/math/2640e9252aac1782adab24797b02ae248a680467.svg" alt="(p)"/>: <img class="math" src="../../_images/math/cb268d86d3e5a0fb9f5f407595933749ad554830.svg" alt="d_i=Q_X\,b_i"/>.</li>
</ol>
</li>
<li>The scalar <img class="math" src="../../_images/math/00664e1095443f6fcb3876d52e62de13c04eea2f.svg" alt="\,A_{i,i}=e_i^T\,A_X\,e_i =e_i^T\,R_X^{-1}\,R_X^{-T}\,e_i = (R_X^{-T}\,e_i)^T \,R_X^{-T}\,e_i"/> depends on <img class="math" src="../../_images/math/720e690c8e7097293116ec591e6077ec1f347afc.svg" alt="x_i"/> the column to delete.
The computation of this scalar is done by <img class="math" src="../../_images/math/bcd61e15b30c959d77d6cdf1581b3e80d60d5604.svg" alt="\,A_{i,i}=b_i^T\,b_i"/> .</li>
</ul>
</div>
<div class="section" id="stepwise-regression-algorithms">
<h4>Stepwise regression algorithms<a class="headerlink" href="#stepwise-regression-algorithms" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li>Input: <img class="math" src="../../_images/math/d2fe8547a74da801e66cd7ce91994c10590da04c.svg" alt="S_0"/>, <img class="math" src="../../_images/math/327ebab5764b7b25928c7c3c021d4b1245205e17.svg" alt="S_{min}"/>, <img class="math" src="../../_images/math/f485fc0c7b5b2f36b76d9dab96a924e13fab1a68.svg" alt="S_{max}"/>, <img class="math" src="../../_images/math/6a021e5a5c0737edb1db932d58fb1b110239d3c2.svg" alt="\mbox{\ttfamily penalty\_} = \{2,\log(n)\}"/>,  <img class="math" src="../../_images/math/1ce9cb8429b61324e8fb0bad8fe67b135bd775af.svg" alt="\mbox{\ttfamily maxIter\_}"/></li>
<li>Initialization: <img class="math" src="../../_images/math/882f2a9af8ddcd2105ed166a0767c4f88fe0e9aa.svg" alt="S^* = S_0"/> , <img class="math" src="../../_images/math/73b17e70dffcce7f489b716f0d047d45ba48125a.svg" alt="X = (x^k)_{k \in S^*}=\mbox{\ttfamily currentX\_}"/>, <img class="math" src="../../_images/math/27af66389b22969e5819d69d1a384934927f80e1.svg" alt="Y=\mbox{\ttfamily Y\_}"/>, <img class="math" src="../../_images/math/20621134530d4d0bd0273a31d14ea3ef55b2fa1d.svg" alt="n_{iter} = 0"/>, <img class="math" src="../../_images/math/ce95b82fa058b6749676338b75534f4224c066a0.svg" alt="X_{max} =\mbox{\ttfamily maxX\_}"/></li>
<li>While (<img class="math" src="../../_images/math/4aa4dcdafd5e26a54f1dac0ca6317bc758735146.svg" alt="n_{iter} &lt; \mbox{\ttfamily maxiter\_}"/>)<ol class="loweralpha">
<li>We compute <img class="math" src="../../_images/math/895dea5ebc41d7ee58d5d769c74ef17efc4d9622.svg" alt="J^* = \log L(\hat{\beta},\hat{\sigma}\mid Y)"/> using  <code class="docutils literal notranslate"><span class="pre">computeLogLikelihood()</span></code> which computes the QR decomposition of matrix <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/>: <img class="math" src="../../_images/math/5b069ec950082afc46663169b5d3df6c344a9e33.svg" alt="Q_X\,R_X=X"/>
and update <img class="math" src="../../_images/math/b54ca61f795292c849037752ebaa24bf32df5f48.svg" alt="Q_X=\,\mbox{\ttfamily currentQ\_}"/>, <img class="math" src="../../_images/math/2321a06dc6b771852783abf49a0edd52cb3cc08b.svg" alt="R_X^{-T} \,=\,\mbox{\ttfamily currentInvRt\_}"/> and
<img class="math" src="../../_images/math/796c22f5207ab34d44d16a3664d140067a60b2c5.svg" alt="\hat{\varepsilon}= Y-\hat{Y}= Y-Q_X\,Q_X^T\,Y =\mbox{\ttfamily currentResidual\_}"/>.</li>
<li>Initialization: <img class="math" src="../../_images/math/3f93b65e0b55e3563f1c52a18ffac0759bba20a5.svg" alt="J^i = +\infty"/>, <img class="math" src="../../_images/math/a1d37f5e8b493c7e5f5bc5d130f23cdc06c30758.svg" alt="J^{i'} = +\infty"/></li>
<li>If (<img class="math" src="../../_images/math/5b5ac5568de2b7d76f40e3499a9785ec7ae262d2.svg" alt="(\mbox{\ttfamily direction\_} \in \big\{ \mbox{\ttfamily FORWARD}, \mbox{\ttfamily BOTH}\big\})"/>), then set
<img class="math" src="../../_images/math/51c6f4102b2e57606726cc7ec8de51f4ac55925a.svg" alt="[\,F_i\,,\,i\,] =  \mbox{\ttfamily computeUpdateForward}(S_{max} \backslash S^*,X_{max},Q_X,Y-\hat{Y})"/> and
<img class="math" src="../../_images/math/40e943694e7c389c33398c24fcd09b6bfba09f5c.svg" alt="J^{i} = n\, \log(\frac{1}{n}F_{{i}})"/></li>
<li>If (<img class="math" src="../../_images/math/8c09c9340381b4d208f351493f4051304477e2ee.svg" alt="(\mbox{\ttfamily direction\_} \in \big\{ \mbox{\ttfamily BACKWARD}, \mbox{\ttfamily BOTH}\big\})"/>), then set
<img class="math" src="../../_images/math/f5291caa6ae8eca05295152cbb2acd859c25cb19.svg" alt="[\,F_{i'}\,,\,{i'}\,] =\mbox{\ttfamily computeUpdateBackward}(S^*\backslash S_{min},X,Y,R_X^{-T},Q_X,Y-\hat{Y})"/> and
<img class="math" src="../../_images/math/2d3307c4b3d8a0586a8ff89dbcc38198bf5031c9.svg" alt="J^{i'} = n\, \log(\frac{1}{n}F_{{i'}})"/></li>
<li>If (<img class="math" src="../../_images/math/eced66d3d5584620f2a1d1d5386394c9c9e1b059.svg" alt="(J^i+\mbox{\ttfamily penalty\_} &lt; J^*)"/> or <img class="math" src="../../_images/math/a18c31a54ce8a955a3dfb440a0e76679807351f4.svg" alt="(J^{i'}-\mbox{\ttfamily penalty\_} &lt; J^*)"/>), then<ol class="lowerroman">
<li>if (<img class="math" src="../../_images/math/eced66d3d5584620f2a1d1d5386394c9c9e1b059.svg" alt="(J^i+\mbox{\ttfamily penalty\_} &lt; J^*)"/>), set <img class="math" src="../../_images/math/feffedacdc770228355834eb377479b94a97bd1b.svg" alt="S^* =S^* \,\cup \, i"/></li>
<li>else set <img class="math" src="../../_images/math/a02dc561187033c9f5f3ec7f9476d183cd433616.svg" alt="S^* =S^* \,\backslash \, i'"/></li>
<li>Set <img class="math" src="../../_images/math/701c016f345112f687a29f4f984bf9e16e725514.svg" alt="X = (x^k)_{k \in S^*}"/></li>
</ol>
</li>
<li>else quit</li>
<li><img class="math" src="../../_images/math/3b1151334a15a2d777da53706c1d1cc8bf9bc551.svg" alt="n_{iter} = n_{iter} + 1"/></li>
</ol>
</li>
<li>We update <img class="math" src="../../_images/math/b54ca61f795292c849037752ebaa24bf32df5f48.svg" alt="Q_X=\,\mbox{\ttfamily currentQ\_}"/>, <img class="math" src="../../_images/math/447f94bb94146a4244144f60d3bbb6ebf891d6c1.svg" alt="R_X^{-T} \,=\mbox{\ttfamily currentInvRt\_}"/>,
<img class="math" src="../../_images/math/153d97e32243a044ccbf380e561147b26d7878d4.svg" alt="\hat{\varepsilon}= Y-\hat{Y}= Y-Q_X\,Q_X^T\,Y \,=\,\mbox{\ttfamily currentResidual\_}"/> using <code class="docutils literal notranslate"><span class="pre">computeLogLikelihood()</span></code></li>
<li>We compute the <img class="math" src="../../_images/math/2640e9252aac1782adab24797b02ae248a680467.svg" alt="(p)"/> vectors: <img class="math" src="../../_images/math/f3a764d8b937259ad479416d60a43dcda07ff23d.svg" alt="\hat{\beta}= A_X\,X^T\,Y= R_X^{-1}\,Q_X^T\,Y"/> and
<img class="math" src="../../_images/math/98e54be459c29142ce63bad0e12bea5a28216a0d.svg" alt="\big(A_{i,i}\big)_{i \in [1,p]} = \big(\,\|R_X^{-T}\,e_i\|^2\,\big)_{i \in [1,p]}"/></li>
<li>We compute the <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> vectors: <img class="math" src="../../_images/math/3e60441473e0c3c536d018947374be1d240e85fc.svg" alt="\big(H_{i,i}\big)_{i \in [1,n]} = \big(\,\|Q_X^{T}\,e_i\|^2\,\big)_{i \in [1,n]}"/> and
<img class="math" src="../../_images/math/f53fc3ed055feef9c45d1fd4e8626ae805bb07e6.svg" alt="\big(D_{i}\big)_{i \in [1,n]} =\Big(\,\frac{(n-1-p)\hat{\varepsilon}_i^2}{p\,\|\hat{\varepsilon}\|^2}\,\frac{H_{i,i}}{(1-H_{i,i})^2}\,\Big)_{i \in [1,n]}"/></li>
<li>We construct an instance of <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelResult.html#otlm.LinearModelResult" title="otlm.LinearModelResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelResult</span></code></a> with parameters: <img class="math" src="../../_images/math/9fd0e4d36c12c432e244455c861b5fddb2ac9aed.svg" alt="\big(\,X,Y,\hat{\varepsilon},(A_{i,i})_{i \in [1,p]},(H_{i,i})_{i \in [1,n]},(D_{i})_{i \in [1,n]}\,\big)"/>.</li>
</ol>
</div>
<div class="section" id="computeupdateforward-algorithm">
<h4><code class="docutils literal notranslate"><span class="pre">ComputeUpdateForward</span></code> algorithm<a class="headerlink" href="#computeupdateforward-algorithm" title="Permalink to this headline">¶</a></h4>
<p>The function <code class="docutils literal notranslate"><span class="pre">computeUpdateForward</span></code> computes the least square of the residual term <img class="math" src="../../_images/math/94bf38995af5025e007a68208f6428c6615e8919.svg" alt="(Y-H_{+i}\,Y)"/> using equation <a class="reference internal" href="#equation-defhpy">(5)</a>:</p>
<ol class="arabic simple">
<li>Input: <img class="math" src="../../_images/math/1e162306bcebe9120f1f0f1c7bfe2e0f85d44e63.svg" alt="S_{max} \backslash S^*"/>,
<img class="math" src="../../_images/math/9b1ba2c4c67c5b444671de6cac6ee525eeef2336.svg" alt="(n \times m)"/> matrix <img class="math" src="../../_images/math/303944df70a034cea2f47819a673a19da15111b0.svg" alt="X_{max}"/>,
<img class="math" src="../../_images/math/21796bef7046b998e5e5889d2713bbe1c38e2f84.svg" alt="(n\times p)"/> matrix <img class="math" src="../../_images/math/1af16b202b92e68b24f79b8bfe037819e282bb24.svg" alt="Q_X"/>, <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> vector <img class="math" src="../../_images/math/b7b1161975a06a808562719906553f4c34253a77.svg" alt="Y-\hat{Y}"/></li>
<li>Initialisation: <img class="math" src="../../_images/math/3d58b9f6c03de3b2ac38354b313a6ab846f07706.svg" alt="F_i = +\infty"/></li>
<li>For (<img class="math" src="../../_images/math/03521c8d6e425fe35c13760fd6469e46753de891.svg" alt="j \in S_{max} \backslash S^*"/>), do<ol class="loweralpha">
<li>We compute the <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> vector <img class="math" src="../../_images/math/1e319009aa13f4d342b4df4df8dedb6f444b99e8.svg" alt="d_{j}-x_j = Q_X\, Q_X^T\,x_j -x_j"/></li>
<li>We compute the <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> vector <img class="math" src="../../_images/math/55d09ea680d46ea15d96b4ef5986cefa82db0735.svg" alt="Y-H_{+j}\,Y = Y-\hat{Y} -\frac{x_j^T\,(Y -\hat{Y})}{x_j^T\,(d_{j}-x_j )} \big(d_{j}-x_j \big)"/></li>
<li>We compute the scalar: <img class="math" src="../../_images/math/bd62da09052c192fecd6a45c1a0267b097a6e540.svg" alt="F_j \hat{=}\|\,Y-H_{+j}\,Y\,\|^2_2"/></li>
<li>If (<img class="math" src="../../_images/math/c6fc565de5ccefa4f768f1a2a51a00c3d031e83c.svg" alt="F_j \, &lt; \, F_i"/>), set <img class="math" src="../../_images/math/40685d190bc7b77ce6ce1b0b7b8cb3ee21201294.svg" alt="F_i\,=\, F_j"/> and <img class="math" src="../../_images/math/b0a67e8e0a20c0f24e9ff3df3d0f6843e786c59b.svg" alt="i=j"/></li>
</ol>
</li>
<li>Return <img class="math" src="../../_images/math/720538a4901599da0f94d7adc19985e12e0495ef.svg" alt="F_i"/> and <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/></li>
</ol>
</div>
<div class="section" id="computeupdatebackward-algorithm">
<h4><code class="docutils literal notranslate"><span class="pre">ComputeUpdateBackward</span></code> algorithm<a class="headerlink" href="#computeupdatebackward-algorithm" title="Permalink to this headline">¶</a></h4>
<p>The function <code class="docutils literal notranslate"><span class="pre">ComputeUpdateBackward</span></code> computes the least square of the residual term  <img class="math" src="../../_images/math/07a5038e0f8906fef0ea816a13ee41e19af2b60a.svg" alt="(Y-H_{-i}\,Y)"/> using equation <a class="reference internal" href="#equation-defhmy2">(9)</a>:</p>
<ol class="arabic simple">
<li>Input: <img class="math" src="../../_images/math/608253cf4aa540ee3c7a89330fb4e00f1a3614a8.svg" alt="S^*\backslash S_{min}"/>,
<img class="math" src="../../_images/math/21796bef7046b998e5e5889d2713bbe1c38e2f84.svg" alt="(n\times p)"/> matrix <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/>,
<img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> vector <img class="math" src="../../_images/math/107a2558bec41a738d186af09dd02b964e85a7bb.svg" alt="Y"/>,
<img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> vector <img class="math" src="../../_images/math/b7b1161975a06a808562719906553f4c34253a77.svg" alt="Y-\hat{Y}"/>
<img class="math" src="../../_images/math/21796bef7046b998e5e5889d2713bbe1c38e2f84.svg" alt="(n\times p)"/> matrix <img class="math" src="../../_images/math/1af16b202b92e68b24f79b8bfe037819e282bb24.svg" alt="Q_X"/>,
<img class="math" src="../../_images/math/a788dede5b50f9ae1e949b74501b9c1bc0a4f783.svg" alt="(p\times p)"/> matrix <img class="math" src="../../_images/math/00b3d17d99cda965995d8b493c69454e5bb7928c.svg" alt="R_X^{-T}"/>,</li>
<li>Initialisation: <img class="math" src="../../_images/math/3d58b9f6c03de3b2ac38354b313a6ab846f07706.svg" alt="F_i = +\infty"/></li>
<li>For (<img class="math" src="../../_images/math/6a5902f187ea9cd1b58abc9e81e931f2b434b33d.svg" alt="j \in S^*\backslash S_{min}"/>), do<ol class="loweralpha">
<li>We compute the <img class="math" src="../../_images/math/2640e9252aac1782adab24797b02ae248a680467.svg" alt="(p)"/> vector <img class="math" src="../../_images/math/bc9ba38907bb42885a2285cda762265190c65de7.svg" alt="b_{j} =\,R_X^{-T}\,e_{j}"/></li>
<li>We compute the <img class="math" src="../../_images/math/2640e9252aac1782adab24797b02ae248a680467.svg" alt="(p)"/> vector <img class="math" src="../../_images/math/758897a276d8fa684d574ffd3b5768db16bad684.svg" alt="d_{j} =\,Q_X\,b_{j}"/></li>
<li>We compute the <img class="math" src="../../_images/math/f58fdccebd96ad22d28b7f82abd8a958c326c91a.svg" alt="(n)"/> vector <img class="math" src="../../_images/math/7b95f601d6da94565d494732eab793c101036682.svg" alt="Y- H_{-j}\,Y\,=\,Y-\hat{Y} \,+\,\frac {d_{j}^T\,Y}{\|\,b_{j}\,\|^2}\,d_{j}\,\big)"/></li>
<li>We compute the scalar: <img class="math" src="../../_images/math/a0c121f4828c464799b867e030ef88fdf49e488a.svg" alt="F_j \hat{=}\|\,Y-H_{-j}\,Y\,\|^2_2"/></li>
<li>If (<img class="math" src="../../_images/math/c6fc565de5ccefa4f768f1a2a51a00c3d031e83c.svg" alt="F_j \, &lt; \, F_i"/>), set <img class="math" src="../../_images/math/40685d190bc7b77ce6ce1b0b7b8cb3ee21201294.svg" alt="F_i\,=\, F_j"/> and <img class="math" src="../../_images/math/b0a67e8e0a20c0f24e9ff3df3d0f6843e786c59b.svg" alt="i=j"/></li>
</ol>
</li>
<li>Return <img class="math" src="../../_images/math/720538a4901599da0f94d7adc19985e12e0495ef.svg" alt="F_i"/> and <img class="math" src="../../_images/math/38caed9f1ca123135271cd7d911e8588015432ed.svg" alt="i"/></li>
</ol>
</div>
</div>
</div>
</div>
<div class="section" id="perspectives">
<h1>Perspectives<a class="headerlink" href="#perspectives" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li>Integration into OpenTURNS<ul>
<li>classes <a class="reference external" href="http://openturns.github.io/user_manual/_generated/openturns.LinearModel.html#openturns.LinearModel" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModel</span></code></a> and <a class="reference external" href="http://openturns.github.io/user_manual/_generated/openturns.LinearModelFactory.html#openturns.LinearModelFactory" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelFactory</span></code></a>
should be fully dropped</li>
<li><em>Tensorization</em>: This module adds the <a class="reference internal" href="../../user_manual/_generated/otlm.MonomialFactory.html#otlm.MonomialFactory" title="otlm.MonomialFactory"><code class="xref py py-class docutils literal notranslate"><span class="pre">MonomialFactory</span></code></a> class to help
creating basis of monomials.  OpenTURNS implements polynomial tensorization for
orthogonal basis.  For this reason, <a class="reference internal" href="../../user_manual/_generated/otlm.MonomialFactory.html#otlm.MonomialFactory" title="otlm.MonomialFactory"><code class="xref py py-class docutils literal notranslate"><span class="pre">MonomialFactory</span></code></a> inherits from
<code class="xref py py-class docutils literal notranslate"><span class="pre">OrthogonalUniVariatePolynomialFactory</span></code>.  But this is wrong, since
monomials do not form an orthogonal basis; polynomial tensorization should be
modified to also generate non-orthogonal basis.</li>
<li>Class <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAlgorithm.html#otlm.LinearModelAlgorithm" title="otlm.LinearModelAlgorithm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelAlgorithm</span></code></a> currently calls <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelStepwiseAlgorithm.html#otlm.LinearModelStepwiseAlgorithm" title="otlm.LinearModelStepwiseAlgorithm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelStepwiseAlgorithm</span></code></a>
to build the linear model.  This is to avoid code duplication when creating a
<a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelResult.html#otlm.LinearModelResult" title="otlm.LinearModelResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelResult</span></code></a>, but this should be fixed.</li>
<li>Drop rot package</li>
</ul>
</li>
<li>Extensions<ul>
<li>Extend <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAnalysis.html#otlm.LinearModelAnalysis" title="otlm.LinearModelAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelAnalysis</span></code></a> to accept <a class="reference external" href="http://openturns.github.io/user_manual/response_surface/_generated/openturns.FunctionalChaosResult.html#openturns.FunctionalChaosResult" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunctionalChaosResult</span></code></a>
as argument.</li>
<li><em>Input normalization</em>: At the moment, inputs are normalized after applying basis’ functions.
To improve robustness, it would be better to normalize input before applying basis’ functions.
But in fact, data should be normalized before performing linear regression.</li>
<li><em>Multivariate output</em>: stepwise selection is currently implemented only when output is 1D.</li>
<li><em>Singular Value Decomposition</em>: algorithm currently uses a QR-decomposition of input sample.
By using a singular value decomposition, maybe some post-processing computations (like
leverages) could be easier to compute.</li>
<li>Instead of optimal trend coefficients, maybe we could return their law.</li>
<li>Extend use of the stepwise method for generalized linear models.</li>
<li><em>Sensitivity analysis</em>: standard regression coefficients are currently defined in
<code class="xref py py-class docutils literal notranslate"><span class="pre">CorrelationAnalysis_SRC</span></code>.
An improvement is to rely on <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelResult.html#otlm.LinearModelResult" title="otlm.LinearModelResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelResult</span></code></a> in a new post-processing.</li>
<li>Extend <a class="reference external" href="http://openturns.github.io/user_manual/_generated/openturns.BoxCoxFactory.html#openturns.BoxCoxFactory" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">BoxCoxFactory</span></code></a> to accept a <a class="reference internal" href="../../user_manual/_generated/otlm.LinearModelAlgorithm.html#otlm.LinearModelAlgorithm" title="otlm.LinearModelAlgorithm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearModelAlgorithm</span></code></a>,
as is done with <a class="reference external" href="http://openturns.github.io/user_manual/response_surface/_generated/openturns.GeneralizedLinearModelResult.html#openturns.GeneralizedLinearModelResult" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">GeneralizedLinearModelResult</span></code></a>.</li>
<li>Rework <a class="reference external" href="http://openturns.github.io/user_manual/_generated/openturns.TestResult.html#openturns.TestResult" title="(in OpenTURNS v.)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TestResult</span></code></a>, it currently uses either pValue or (1-pValue).</li>
<li>Improve validation.  We could not validate by comparing with R <code class="docutils literal notranslate"><span class="pre">step</span></code> method because
it filters variables: it would accept <code class="docutils literal notranslate"><span class="pre">X1*X2</span></code> only after <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code> belong to
the model.  According to literature, this is called the principle of marginality.
Moreover, there are cases where it switches variables, say <code class="docutils literal notranslate"><span class="pre">X2*X1</span></code>, and
it afterwards reject it because it did not match <code class="docutils literal notranslate"><span class="pre">X1*X2</span></code>.</li>
</ul>
</li>
</ul>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Linear regression models</a></li>
<li><a class="reference internal" href="#architecture-considerations">Architecture considerations</a><ul>
<li><a class="reference internal" href="#dependencies">Dependencies</a></li>
<li><a class="reference internal" href="#compilation">Compilation</a></li>
<li><a class="reference internal" href="#source-code-structure">Source code structure</a><ul>
<li><a class="reference internal" href="#linearmodel">LinearModel</a></li>
<li><a class="reference internal" href="#anova-table">ANOVA table</a></li>
<li><a class="reference internal" href="#graphical-diagnostics">Graphical diagnostics</a></li>
<li><a class="reference internal" href="#stepwise-regression-methods">Stepwise regression methods</a><ul>
<li><a class="reference internal" href="#forward-selection">Forward selection</a></li>
<li><a class="reference internal" href="#backward-selection">Backward selection</a></li>
<li><a class="reference internal" href="#bidirectional-selection">Bidirectional selection</a></li>
</ul>
</li>
<li><a class="reference internal" href="#detailed-implementation">Detailed implementation</a><ul>
<li><a class="reference internal" href="#qr-decomposition-of-matrix-x">QR decomposition of matrix <img class="math" src="../../_images/math/72bb27e495afbd8b5f66fa2d0d216098a05972b3.svg" alt="X"/></a></li>
<li><a class="reference internal" href="#id1">Forward selection</a></li>
<li><a class="reference internal" href="#implementation-using-qr-decomposition">Implementation using QR decomposition</a></li>
<li><a class="reference internal" href="#id2">Backward selection</a></li>
<li><a class="reference internal" href="#id3">Implementation using QR decomposition</a></li>
<li><a class="reference internal" href="#stepwise-regression-algorithms">Stepwise regression algorithms</a></li>
<li><a class="reference internal" href="#computeupdateforward-algorithm"><code class="docutils literal notranslate"><span class="pre">ComputeUpdateForward</span></code> algorithm</a></li>
<li><a class="reference internal" href="#computeupdatebackward-algorithm"><code class="docutils literal notranslate"><span class="pre">ComputeUpdateBackward</span></code> algorithm</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#perspectives">Perspectives</a></li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../developer_guide.html"
                        title="previous chapter">Developer guide</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../validation/validation.html"
                        title="next chapter">Validation</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/developer_guide/architecture/architecture.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../validation/validation.html" title="Validation"
             >next</a> |</li>
        <li class="right" >
          <a href="../developer_guide.html" title="Developer guide"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">otlm 0.6 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../developer_guide.html" >Developer guide</a> &#187;</li> 
      </ul>
    </div>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>

  </body>
</html>
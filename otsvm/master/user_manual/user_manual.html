
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>User manual &#8212; OTTemplate 0.0 documentation</title>
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NormalRBF" href="_generated/otsvm.NormalRBF.html" />
    <link rel="prev" title="OTSVM documentation" href="../index.html" /> 
  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="_generated/otsvm.NormalRBF.html" title="NormalRBF"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="OTSVM documentation"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">OTTemplate 0.0 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="user-manual">
<h1>User manual<a class="headerlink" href="#user-manual" title="Permalink to this headline">¶</a></h1>
<p>The Libsvm library provides efficient algorithms to produce a model by Support Vector Machine.</p>
<div class="section" id="introduction-to-support-vector-machine">
<h2>Introduction to support vector machine<a class="headerlink" href="#introduction-to-support-vector-machine" title="Permalink to this headline">¶</a></h2>
<p>A support vector machine is a concept in statistics and computer science for a
set of related supervised learning methods that analyze data and recognize
patterns, used for classification and regression analysis.
The standard SVM takes a set of input data and predicts, for each given input,
which of two possible classes forms the input. Given a set of training
examples, each marked as belonging to one of two categories, a SVM training
algorithm builds a model that assigns new examples into one category or
the other.</p>
<p>More formally, a SVM constructs a hyperplane in a high or infinite-dimensional
space, which can be used for classification or regression. A good separation
is achived by the hyperplane that has the largest distance to the nearest
training data point of any class.</p>
<p>Whereas the original problem may be stated in a finite dimensional space,
it often happens that the sets to discriminate are not linearly separable in
that space. For this reason, it was proposed that the original
finite-dimensional space be mapped into a much higher-dimensional space,
presumably making the separation easier in that space. To keep the
computational load reasonable, the mappings used by SVM schemes are designed to
ensure that dot products may be computed easily in terms of the variables in
the original space, by defining them in terms of a kernel function
<img class="math" src="../_images/math/2d25ee530c2d85c5196fa6c4e16c1871761a5ee2.svg" alt="K(x,y)"/> selected to suit the problem.</p>
</div>
<div class="section" id="linear-svm">
<h2>Linear SVM<a class="headerlink" href="#linear-svm" title="Permalink to this headline">¶</a></h2>
<p>Given some training data <img class="math" src="../_images/math/020f024ee02c8fe77cd8ee99889cdd1132171527.svg" alt="D"/>, a set of n points of the form</p>
<div class="math">
<p><img src="../_images/math/8ea4ce1e72d04590a94dc4fbebda222f64359133.svg" alt="D\ =\ \{(x_{i},y_{i})\ |\ x_{i} \in \mathbb{R}^{p},\ y_{i} \in \{-1,1\}\}^{n}_{i=1}"/></p>
</div><p>where the <img class="math" src="../_images/math/71049efbec155ea4d5a7be5c0fb7ada7703b5c23.svg" alt="y_i"/> is either 1 or -1, indicating the class to which the point
<img class="math" src="../_images/math/142bc3574a112cc42f8645dc406254b93364b9c8.svg" alt="x_i"/> belongs. Each <img class="math" src="../_images/math/59a4cb8abf9f53cfd6738f22332475f28ced199f.svg" alt="x_{i}"/> is a p-dimensional real vector.
We want to find the maximum-margin hyperplane that divides the points having
<img class="math" src="../_images/math/8945ce0ac1db98ece91762a5160d2bd295fe6697.svg" alt="y_i=1"/> from those having <img class="math" src="../_images/math/14b790a3f376ea6a6e15a864ad62dfae88ce7213.svg" alt="y_i=-1"/>.</p>
<p>Any hyperplane can be written as the set of points <img class="math" src="../_images/math/65e00ef15f56013de8f626c7b61fd8f0fc3c66ea.svg" alt="x"/> satisfying
<img class="math" src="../_images/math/00dbbe229e3539c895e3660188b039908bb98259.svg" alt="w \cdot x - b = 0"/> where <img class="math" src="../_images/math/786495b2f7cc198288fb46efa8236488b60ae6d8.svg" alt="cdot"/> denotes the dot product and
<img class="math" src="../_images/math/a5870c7ef7cde1ab15b8f6e65fb3c2955c82f870.svg" alt="w"/> the normal vector to the hyperplane.
We want to choose <img class="math" src="../_images/math/a5870c7ef7cde1ab15b8f6e65fb3c2955c82f870.svg" alt="w"/> and <img class="math" src="../_images/math/eb47c3579f7ee5fccd81ae14ceda0ab5e47db07c.svg" alt="b"/> to maximize the margin, or distance
between the parallel hyperplanes that are as far apart as possible while still
separating the data. These hyperplanes can be described by the equations</p>
<div class="math">
<p><img src="../_images/math/6a203f7205453475eb09817d15e80de893d0dbe3.svg" alt="w \cdot x - b = 1"/></p>
</div><p>and</p>
<div class="math">
<p><img src="../_images/math/51408eb48d9644a9eb671b0b070168d4ddc2f7a0.svg" alt="w \cdot x - b = -1"/></p>
</div><p>The distance between these two hyperplanes is <img class="math" src="../_images/math/d8eca5d1d82032ba26cf187a467a44c5a6927f1e.svg" alt="\frac{2}{||w||}"/>, so we
want to minimize <img class="math" src="../_images/math/edb4c750030e39fb012421fb0373a78424a6f0bb.svg" alt="||w||"/>. As we also have to prevent data points from
falling into the margin, we add the following constraint : for each i either</p>
<div class="math">
<p><img src="../_images/math/c5378bb137a9c48b65d7289b85cdaa9acbe63d7c.svg" alt="y_{i}(w \cdot x_{i} - b)\geq 1\quad for\ all\ 1\leq i\leq n"/></p>
</div><p>So we get the optimization problem :</p>
<div class="math">
<p><img src="../_images/math/7ad88b13eeb69474909ff8cb57a4dc783fd6f18e.svg" alt="Min\ ||w||
subject\ to\ ( for\ any\ i=1,...,n )
y_{i}(w \cdot x_{i} - b)\geq 1"/></p>
</div></div>
<div class="section" id="primal-form">
<h2>Primal form<a class="headerlink" href="#primal-form" title="Permalink to this headline">¶</a></h2>
<p>The optimization problem presented in the preceding section is difficult to
solve because it depends on <img class="math" src="../_images/math/edb4c750030e39fb012421fb0373a78424a6f0bb.svg" alt="||w||"/>, which involves a
square root. It is possible to alter the equation by substituing <img class="math" src="../_images/math/edb4c750030e39fb012421fb0373a78424a6f0bb.svg" alt="||w||"/>
with <img class="math" src="../_images/math/d21a9771e855b36947bb35b12b0040c0758e871f.svg" alt="\frac{1}{2}||w||^{2}"/>
without changing the solution. This is a quadratic programming optimization problem:</p>
<div class="math">
<p><img src="../_images/math/8b17f4cc95292de93b320e4c05d3bc9682cb8be3.svg" alt="Min\ \frac{1}{2}||w||^{2}
subject\ to\ ( for\ any\ i=1,...,n )
y_{i}(w \cdot x_{i} - b)\geq 1"/></p>
</div><p>By introducing Lagrange multipliers <img class="math" src="../_images/math/3e6c782b9991bcd262b8e34715ed55fcc23f635c.svg" alt="\alpha"/>, the previous constrained problem can be expressed as</p>
<div class="math">
<p><img src="../_images/math/20324f4a06b862f976cd3e37e2950cc5495a9d0a.svg" alt="\underset{w,b}{\text{min}}\ \underset{\alpha \geq 0}{\text{max}}\{\frac{1}{2}||w||^{2}-\sum_{i=1}^n \alpha_{i}[y_{i}(w \cdot x_{i} - b)-1]\}"/></p>
</div><p>This problem can now be solved by standard quadratic programming techniques and
programs. The stationary Karush-Kuhn-Tucker condition implies that the solution
can be expressed as a linear combination of the training vectors
<img class="math" src="../_images/math/3f944ddcff2d01f855dcc9cd3d0f529e9edec1aa.svg" alt="w = \sum_{i=1}^n \alpha_i y_i x_i"/>.</p>
<p>Only a few <img class="math" src="../_images/math/6f828a01f05b7b51ce3a93075e8c8ca2c227e9a9.svg" alt="\alpha_i"/> will be greater than zero. The corresponding
<img class="math" src="../_images/math/142bc3574a112cc42f8645dc406254b93364b9c8.svg" alt="x_i"/> are exactly the support vectors, which lie on the margin and
satisfy <img class="math" src="../_images/math/14fa038faad6e64fc368cfa1aaa5c61ad2066042.svg" alt="y_i (w\cdot x_i -b)=1"/>.</p>
</div>
<div class="section" id="dual-form">
<h2>Dual form<a class="headerlink" href="#dual-form" title="Permalink to this headline">¶</a></h2>
<p>Using the fact that <img class="math" src="../_images/math/9c12e0a6db033f891b1df5645e67728aa38c4c2d.svg" alt="||w||^2 = w \cdot w"/> and substituing
<img class="math" src="../_images/math/b042377496a0ab357150da528b6e57d48fb7bc02.svg" alt="w = \sum_{i=1}^n \alpha_i x_i y_i"/>, one can show that the dual of the
SVM reduces to the following optimization problem:</p>
<div class="math">
<p><img src="../_images/math/f59baec67d5cf37883f4b0040e07d2cb9881fb5a.svg" alt="Max\ L(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j k(x_i,x_j)

subject\ to\ ( for\ any\ i = 1,...,n  )
\alpha_i \geq 0"/></p>
</div><p>and to the constraint from the minimization in b</p>
<div class="math">
<p><img src="../_images/math/531b810f36daa27cf2051b3abf3566100131c522.svg" alt="\sum_{i=1}^n \alpha_i y_i =0"/></p>
</div><p><img class="math" src="../_images/math/a5870c7ef7cde1ab15b8f6e65fb3c2955c82f870.svg" alt="w"/> can be computed thanks to the <img class="math" src="../_images/math/3e6c782b9991bcd262b8e34715ed55fcc23f635c.svg" alt="\alpha"/> terms: <img class="math" src="../_images/math/6d0e5a044622157149040d36ee7910ef562228d9.svg" alt="w=\sum_i \alpha_i y_i x_i"/></p>
</div>
<div class="section" id="soft-margin">
<h2>Soft margin<a class="headerlink" href="#soft-margin" title="Permalink to this headline">¶</a></h2>
<p>If there exists no hyperplane that can split the “yes” and “no” examples, the
soft margin method will choose a hyperplane that splits the examples as cleanly
as possible, while still maximizing the distance to the nearest cleanly split
examples. The method introduces slack variables, <img class="math" src="../_images/math/f3fc04996f9ecb15c4d39c9ede115d99ca6b69fa.svg" alt="\xi_i"/>, which measure the
degree of misclassification of the data <img class="math" src="../_images/math/142bc3574a112cc42f8645dc406254b93364b9c8.svg" alt="x_i"/></p>
<div class="math">
<p><img src="../_images/math/45c11f8255b7b5410aebacfe093ae490062b8be4.svg" alt="y_i(w \cdot x_i - b) \geq 1 - \xi_i \quad 1 \leq i \leq n"/></p>
</div><p>The objective function is then increased by a function which penalizes non-zero
<img class="math" src="../_images/math/f3fc04996f9ecb15c4d39c9ede115d99ca6b69fa.svg" alt="\xi_i"/> and the optimization becomes a trade off between a large margin
and a small error penalty. If the penalty function is linear, the optimization
problem becomes:</p>
<div class="math">
<p><img src="../_images/math/61a0bc667582c0f03f9c9dc1f7a0492d67548e1d.svg" alt="\underset{w,b,\xi}{\text{min}}\{\frac{1}{2} ||w||^2 + C\sum_{i=1}^n \xi_i \}
subject to (for any i=1,...,n)
y_i ( w \cdot x_i - b) \geq 1 - \xi_i \quad \xi_i \geq 0"/></p>
</div><p>This constaint with the objective of minimizing <img class="math" src="../_images/math/edb4c750030e39fb012421fb0373a78424a6f0bb.svg" alt="||w||"/> can be solved
using Lagrange multipliers as done above. One has then to solve the problem:</p>
<div class="math">
<p><img src="../_images/math/0637110bc0e6d415e0d63aa8cbeeb49de795833d.svg" alt="\underset{w,b,\xi}{\text{min}}\ \underset{\alpha,\beta}{\text{max}}\{\frac{1}{2} ||w||^2 + C\sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i[y_i(w \cdot x_i - b)-1 +\xi_i]- \sum_{i=1}^n \beta_i\xi_i \}
with \alpha_i , \beta_i \geq 0"/></p>
</div><p>The dual form becomes:</p>
<div class="math">
<p><img src="../_images/math/191f396d9daae9c3202fa9ddc2f8a4e24a3b778a.svg" alt="\underset{\alpha_i}{\text{max}}\ \{L(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j k(x_i,x_j)\}
subject to (for any i=1,...,n)
0 \leq \alpha_i \leq C
and
\sum_{i=1}^n \alpha_i y_i = 0"/></p>
</div></div>
<div class="section" id="nonlinear-svm">
<h2>Nonlinear SVM<a class="headerlink" href="#nonlinear-svm" title="Permalink to this headline">¶</a></h2>
<p>The algorithm is formally similar, except that every dot product is replaced by
a nonlinear kernel function. This allows the algorithm to fit the
maximum-margin hyperplane in a transformed feature space.
The transformation may be nonlinear and the transformed space high dimensional,
thus though the classifier is a hyperplane in the high-dimensional feature
space, it may be nonlinear in the original input space.</p>
<dl class="docutils">
<dt>Some common kernels include:</dt>
<dd><ul class="first last simple">
<li>Polynomial : <img class="math" src="../_images/math/f52d396a89c62a15284918f61312c3a5f16d8418.svg" alt="k(x_i,x_j)=(x_i\cdot x_j+c)^d"/></li>
<li>Gaussian Radial Basis Function : <img class="math" src="../_images/math/c1a1d95768a26f7f02b8fe5ecee0e4053958ce1d.svg" alt="k(x_i,x_j)=exp(-\gamma ||x_i-x_j||^2)"/></li>
<li>Hyperbolic tangent : <img class="math" src="../_images/math/443ba99c5b73fa42f73a4227edc024527c151201.svg" alt="k(x_i,x_j)=tanh(\gamma x_i\cdot x_j + c)"/></li>
</ul>
</dd>
</dl>
<p>The kernel is related to the transform <img class="math" src="../_images/math/45aaf8c8dd02fadc35bd3a14507e39213d8cfa12.svg" alt="\varphi(x_i)"/> by the equation</p>
<div class="math">
<p><img src="../_images/math/7fae7511283b6f54350467054dbd1aa05e68a450.svg" alt="k(x_i,x_j)=\varphi(x_i)\cdot\varphi(x_j)"/></p>
</div><p>The value <img class="math" src="../_images/math/a5870c7ef7cde1ab15b8f6e65fb3c2955c82f870.svg" alt="w"/> is also in the transformed space, with <img class="math" src="../_images/math/73b2902d668e348e2bc430224c2218954bf83bf0.svg" alt="w=\sum_i\alpha_i y_i\varphi(x_i)"/>.</p>
<p>The effectiveness of SVM depends on the selection of kernel, the kernel’s
parameters, and soft margin parameter C. A common choice is a Gaussian kernel,
which has a single parameter <img class="math" src="../_images/math/f211680132992c60e9ac7cdd79136a863f28af66.svg" alt="\gamma"/>. Best combination of C and
<img class="math" src="../_images/math/f211680132992c60e9ac7cdd79136a863f28af66.svg" alt="\gamma"/> is selected by a grid search with exponentially growing
sequences of C and <img class="math" src="../_images/math/f211680132992c60e9ac7cdd79136a863f28af66.svg" alt="\gamma"/>. Each combination of parameter choices is
checked using cross validation, and the parameters with best cross-validation
accuracy are picked.
The final model, which is used for testing and for classifying data, is then
trained on the whole training set using the selected parameters.</p>
</div>
<div class="section" id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<p>Given training vectors <img class="math" src="../_images/math/142bc3574a112cc42f8645dc406254b93364b9c8.svg" alt="x_i"/> in two classes and a vector <img class="math" src="../_images/math/1cbaabebb964aae14a932ac92805dca8f7e7795e.svg" alt="y\in{-1,1}"/>,
C-SVC (the algorithm that we use for classification) solves the following dual
problem:</p>
<div class="math">
<p><img src="../_images/math/24bedf567d959d4bd97dd11f85536ae9107f164a.svg" alt="min_{\alpha} \frac{1}{2} \alpha^TQ\alpha-e^T\alpha
0\leq \alpha_i \leq C
y^T\alpha=0"/></p>
</div><p>where e is the vector of all ones, <img class="math" src="../_images/math/703d6304eb1b0b1cc953acb7c2388b017e1de6b1.svg" alt="C&gt;0"/> is the upper bound, Q is an
l by l positive semidefinite matrix <img class="math" src="../_images/math/c6ec2e73ad90e97cfae80027d8ecf3cbb86cdd95.svg" alt="Q_{ij}=y_iy_jK(x_i,x_j)"/>
and <img class="math" src="../_images/math/d868404af4d7b94392ee23bd013e859b7e9e55e4.svg" alt="K(x_i,x_j)"/> is the kernel. The decision function is</p>
<div class="math">
<p><img src="../_images/math/10df703455918b26277f2fcaf0ee9e500208465c.svg" alt="sign(\sum_{i=1}^l y_i\alpha_iK(x_i,x)+b)"/></p>
</div><p>For some classification problems, numbers of data in different classes are
unbalanced. We can use different penalty parameters in the SVM formulation,
the dual of C-SVC becomes:</p>
<div class="math">
<p><img src="../_images/math/9e0c374d5d9388d0ef172824744cd3e53c711e1a.svg" alt="min_{\alpha} \frac{1}{2} \alpha^TQ\alpha-e^T\alpha
0\leq \alpha_i \leq C_+,\ if\ y_i=1
0\leq \alpha_i \leq C_-,\ if\ y_i=-1
y^T\alpha=0"/></p>
</div><p>where <img class="math" src="../_images/math/6e0fbd505b3d9da681eee286254d1ac4a952ee49.svg" alt="C_+"/> and <img class="math" src="../_images/math/436c6d96e2472b39be36dd84375edd0986d045d0.svg" alt="C_-"/> depending on <img class="math" src="../_images/math/71049efbec155ea4d5a7be5c0fb7ada7703b5c23.svg" alt="y_i"/> and <img class="math" src="../_images/math/285664f3b61c1b1d53a7f6dfe5a6e261385c84c7.svg" alt="y_j"/>
and of weights that we can fix for each class.</p>
</div>
<div class="section" id="regression">
<h2>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<p>Up to now, we presented SVM for classification. We can use too for regression.
This is similar to the nonlinear case. We replace the soft margin by
a <img class="math" src="../_images/math/546c91e3c3cc4138b92374ff4ede9c7704e36344.svg" alt="\varepsilon"/>-insensitive loss function which is defined like:</p>
<div class="math">
<p><img src="../_images/math/9a20de080e4bdd67d326f7a55d677950213b8414.svg" alt="|y-f(x)|_\varepsilon = max(0,|y-f(x)|-\varepsilon)"/></p>
</div><p>where f(x) is the loss function and <img class="math" src="../_images/math/546c91e3c3cc4138b92374ff4ede9c7704e36344.svg" alt="\varepsilon"/> a precision parameter.</p>
<p>We get this optimization problem if we introduce the slack variables <img class="math" src="../_images/math/6b27993236fc82e074ca358e5155624f11ef9b3b.svg" alt="\xi\ and\ \xi_i"/>:</p>
<div class="math">
<p><img src="../_images/math/0ceeed13b0ca7847e59b30449e056f63f1854dc6.svg" alt="min_w \{\frac{||w||^2}{2}+C\sum_{i=1}^n(\xi_i+\xi_i^*) \}
subject to (for any i=1,...,n)
l_i-wx_i+b\leq \varepsilon+\xi_i
wx_i-b-l_i\leq \varepsilon +\xi_i^*
\xi_i,\xi_i^* \geq 0"/></p>
</div><p>with C which is a control parameter like in soft margin.</p>
<p>To solve this problem, we introduce a new time Lagrange multipliers and we
will get this regression function:</p>
<div class="math">
<p><img src="../_images/math/9e9efe264b1b81c72af47e4db9fed0074142a1c5.svg" alt="f(x)=\sum_{i=1}^n(\alpha_i-\alpha_i^*)K(x_i,x)-b"/></p>
</div></div>
<div class="section" id="choose-a-kernel">
<h2>Choose a kernel<a class="headerlink" href="#choose-a-kernel" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_generated/otsvm.NormalRBF.html#otsvm.NormalRBF" title="otsvm.NormalRBF"><code class="xref py py-obj docutils literal"><span class="pre">NormalRBF</span></code></a>(*args)</td>
<td><p class="rubric">Methods</p>
</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="_generated/otsvm.ExponentialRBF.html#otsvm.ExponentialRBF" title="otsvm.ExponentialRBF"><code class="xref py py-obj docutils literal"><span class="pre">ExponentialRBF</span></code></a>(*args)</td>
<td><p class="rubric">Methods</p>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="_generated/otsvm.LinearKernel.html#otsvm.LinearKernel" title="otsvm.LinearKernel"><code class="xref py py-obj docutils literal"><span class="pre">LinearKernel</span></code></a>(*args)</td>
<td><p class="rubric">Methods</p>
</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="_generated/otsvm.PolynomialKernel.html#otsvm.PolynomialKernel" title="otsvm.PolynomialKernel"><code class="xref py py-obj docutils literal"><span class="pre">PolynomialKernel</span></code></a>(*args)</td>
<td><p class="rubric">Methods</p>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="_generated/otsvm.RationalKernel.html#otsvm.RationalKernel" title="otsvm.RationalKernel"><code class="xref py py-obj docutils literal"><span class="pre">RationalKernel</span></code></a>(*args)</td>
<td><p class="rubric">Methods</p>
</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="_generated/otsvm.SigmoidKernel.html#otsvm.SigmoidKernel" title="otsvm.SigmoidKernel"><code class="xref py py-obj docutils literal"><span class="pre">SigmoidKernel</span></code></a>(*args)</td>
<td><p class="rubric">Methods</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id1">
<h2>Classification<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_generated/otsvm.LibSVMClassification.html#otsvm.LibSVMClassification" title="otsvm.LibSVMClassification"><code class="xref py py-obj docutils literal"><span class="pre">LibSVMClassification</span></code></a>(*args)</td>
<td><p class="rubric">Methods</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id2">
<h2>Regression<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_generated/otsvm.LibSVMRegression.html#otsvm.LibSVMRegression" title="otsvm.LibSVMRegression"><code class="xref py py-obj docutils literal"><span class="pre">LibSVMRegression</span></code></a>(*args)</td>
<td><p class="rubric">Methods</p>
</td>
</tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">User manual</a><ul>
<li><a class="reference internal" href="#introduction-to-support-vector-machine">Introduction to support vector machine</a></li>
<li><a class="reference internal" href="#linear-svm">Linear SVM</a></li>
<li><a class="reference internal" href="#primal-form">Primal form</a></li>
<li><a class="reference internal" href="#dual-form">Dual form</a></li>
<li><a class="reference internal" href="#soft-margin">Soft margin</a></li>
<li><a class="reference internal" href="#nonlinear-svm">Nonlinear SVM</a></li>
<li><a class="reference internal" href="#classification">Classification</a></li>
<li><a class="reference internal" href="#regression">Regression</a></li>
<li><a class="reference internal" href="#choose-a-kernel">Choose a kernel</a></li>
<li><a class="reference internal" href="#id1">Classification</a></li>
<li><a class="reference internal" href="#id2">Regression</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../index.html"
                        title="previous chapter">OTSVM documentation</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="_generated/otsvm.NormalRBF.html"
                        title="next chapter">NormalRBF</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/user_manual/user_manual.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="_generated/otsvm.NormalRBF.html" title="NormalRBF"
             >next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="OTSVM documentation"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">OTTemplate 0.0 documentation</a> &#187;</li> 
      </ul>
    </div>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>

  </body>
</html>